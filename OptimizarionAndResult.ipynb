{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **COBEQ 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "import optuna\n",
    "import itertools\n",
    "import shutil\n",
    "from functools import partial\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.subplots\n",
    "import time\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NBEATS, NHITS\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "ponte = pd.read_pickle(r'Data\\Data_Ponte_dos_Remedios.pkl')\n",
    "del ponte['o3']\n",
    "guarulhos = pd.read_pickle(r'Data\\Data_Guarulhos.pkl')\n",
    "guarulhos = guarulhos[['date','o3']]\n",
    "\n",
    "data = ponte.merge(guarulhos, on='date', how='outer')\n",
    "data.reset_index(drop=True)\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "os.environ['NIXTLA_ID_AS_COL'] = '1'\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "trainer = Trainer(\n",
    "    max_steps=4,\n",
    "    logger=False,\n",
    "    enable_progress_bar=False,\n",
    "    enable_model_summary=False  # Disable model summary\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"optuna\")\n",
    "\n",
    "from TimeObjectModule import TimeObject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Experimental Planning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in [14, 30, 60, 90, 120]:\n",
    "    for k in [1, 2, 3, 4, 5,]:\n",
    "        x = TimeObject(data, column='pm10', agg_freq='D')\n",
    "        x.fixed_origin_rolling_window_cross_validation(\n",
    "            initial_train_date=0,\n",
    "            total_data_points=1000+(k*h),\n",
    "            h=h,\n",
    "            # plot_interval=True,\n",
    "            width=1100,height=400\n",
    "        )\n",
    "        # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H = 14\n",
      "Fold: 1 | Train = 2452 | Percent = 99.432\n",
      "Fold: 2 | Train = 2438 | Percent = 98.865\n",
      "Fold: 3 | Train = 2424 | Percent = 98.297\n",
      "Fold: 4 | Train = 2410 | Percent = 97.729\n",
      "Fold: 5 | Train = 2396 | Percent = 97.161\n",
      "=================\n",
      "Fold: 1 | Train = 2382 | Percent = 96.594\n",
      "Fold: 2 | Train = 2368 | Percent = 96.026\n",
      "Fold: 3 | Train = 2354 | Percent = 95.458\n",
      "Fold: 4 | Train = 2340 | Percent = 94.891\n",
      "Fold: 5 | Train = 2326 | Percent = 94.323\n",
      "\n",
      "\n",
      "H = 30\n",
      "Fold: 1 | Train = 2436 | Percent = 98.783\n",
      "Fold: 2 | Train = 2406 | Percent = 97.567\n",
      "Fold: 3 | Train = 2376 | Percent = 96.35\n",
      "Fold: 4 | Train = 2346 | Percent = 95.134\n",
      "Fold: 5 | Train = 2316 | Percent = 93.917\n",
      "=================\n",
      "Fold: 1 | Train = 2286 | Percent = 92.701\n",
      "Fold: 2 | Train = 2256 | Percent = 91.484\n",
      "Fold: 3 | Train = 2226 | Percent = 90.268\n",
      "Fold: 4 | Train = 2196 | Percent = 89.051\n",
      "Fold: 5 | Train = 2166 | Percent = 87.835\n",
      "\n",
      "\n",
      "H = 60\n",
      "Fold: 1 | Train = 2406 | Percent = 97.567\n",
      "Fold: 2 | Train = 2346 | Percent = 95.134\n",
      "Fold: 3 | Train = 2286 | Percent = 92.701\n",
      "Fold: 4 | Train = 2226 | Percent = 90.268\n",
      "Fold: 5 | Train = 2166 | Percent = 87.835\n",
      "=================\n",
      "Fold: 1 | Train = 2106 | Percent = 85.401\n",
      "Fold: 2 | Train = 2046 | Percent = 82.968\n",
      "Fold: 3 | Train = 1986 | Percent = 80.535\n",
      "Fold: 4 | Train = 1926 | Percent = 78.102\n",
      "Fold: 5 | Train = 1866 | Percent = 75.669\n",
      "\n",
      "\n",
      "H = 90\n",
      "Fold: 1 | Train = 2376 | Percent = 96.35\n",
      "Fold: 2 | Train = 2286 | Percent = 92.701\n",
      "Fold: 3 | Train = 2196 | Percent = 89.051\n",
      "Fold: 4 | Train = 2106 | Percent = 85.401\n",
      "Fold: 5 | Train = 2016 | Percent = 81.752\n",
      "=================\n",
      "Fold: 1 | Train = 1926 | Percent = 78.102\n",
      "Fold: 2 | Train = 1836 | Percent = 74.453\n",
      "Fold: 3 | Train = 1746 | Percent = 70.803\n",
      "Fold: 4 | Train = 1656 | Percent = 67.153\n",
      "Fold: 5 | Train = 1566 | Percent = 63.504\n",
      "\n",
      "\n",
      "H = 120\n",
      "Fold: 1 | Train = 2346 | Percent = 95.134\n",
      "Fold: 2 | Train = 2226 | Percent = 90.268\n",
      "Fold: 3 | Train = 2106 | Percent = 85.401\n",
      "Fold: 4 | Train = 1986 | Percent = 80.535\n",
      "Fold: 5 | Train = 1866 | Percent = 75.669\n",
      "=================\n",
      "Fold: 1 | Train = 1746 | Percent = 70.803\n",
      "Fold: 2 | Train = 1626 | Percent = 65.937\n",
      "Fold: 3 | Train = 1506 | Percent = 61.071\n",
      "Fold: 4 | Train = 1386 | Percent = 56.204\n",
      "Fold: 5 | Train = 1266 | Percent = 51.338\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for horizon in [14, 30, 60, 90, 120]:\n",
    "    print(f'H = {horizon}')\n",
    "    max_k_fold = 5\n",
    "    for k_fold in range(0,max_k_fold):\n",
    "        obj = TimeObject(df=data, column='pm10', agg_freq='D')\n",
    "        obj.fixed_origin_rolling_window_cross_validation(\n",
    "            initial_train_date=None,\n",
    "            total_data_points=(len(obj.nixtla_df))-(k_fold*horizon),\n",
    "            h=horizon,\n",
    "            # plot_interval=True\n",
    "        )\n",
    "        print(f'Fold: {k_fold+1} | Train = {len(obj.Y_train)} | Percent = {round(100*len(obj.Y_train)/len(obj.nixtla_df),3)}')\n",
    "    print('=================')\n",
    "    for k_fold in range(0,max_k_fold):\n",
    "        obj = TimeObject(df=data, column='pm10', agg_freq='D')\n",
    "        obj.fixed_origin_rolling_window_cross_validation(\n",
    "            initial_train_date=None,\n",
    "            total_data_points=(len(obj.nixtla_df)-(max_k_fold*horizon))-(k_fold*horizon),\n",
    "            h=horizon,\n",
    "            # plot_interval=True\n",
    "        )\n",
    "        print(f'Fold: {k_fold+1} | Train = {len(obj.Y_train)} | Percent = {round(100*len(obj.Y_train)/len(obj.nixtla_df),3)}')\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Optuna**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-HiTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-12 15:17:37,389] Trial 199 finished with values: [39.045902, 9.382224] and parameters: {'input_size': 633, 'n_stacks': 3, 'n_blocks': 7, 'max_steps': 87, 'local_scalar_type': 'minmax', 'n_pool_kernel_size': [3, 3, 3], 'n_freq_downsample': [1, 1, 1]}.\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function\n",
    "def objective(trial, pollutant, horizon):\n",
    "    # Hyperparameter search space\n",
    "    input_size = trial.suggest_int('input_size', 90, 1100, step=1)\n",
    "    n_stacks = trial.suggest_int('n_stacks', 3, 7, step=1)\n",
    "    n_blocks = trial.suggest_int('n_blocks', 1, 7, step=1)\n",
    "    max_steps = trial.suggest_int('max_steps', 10, 700, step=1)\n",
    "    local_scalar_type = trial.suggest_categorical('local_scalar_type', [None, 'standard', 'boxcox', 'minmax'])\n",
    "    n_pool_kernel_size = trial.suggest_categorical('n_pool_kernel_size', [list(combination) for combination in list(itertools.product([1, 2, 3], repeat=3))])\n",
    "    n_freq_downsample = trial.suggest_categorical('n_freq_downsample', [list(combination) for combination in list(itertools.product([1, 4, 12, 52], repeat=3))])\n",
    "\n",
    "    mape = []\n",
    "    smape = []\n",
    "    max = []\n",
    "    mae = []\n",
    "    mse = []\n",
    "    # Split for cross validation\n",
    "    max_k_fold = 5\n",
    "    for k_fold in range(0,max_k_fold):\n",
    "        print(f'\\nPollutant = {pollutant} \\nh = {horizon} \\nTrial = {trial.number+1}\\nFold = {k_fold+1}\\n')\n",
    "        # Instantiate TimeObject and prepare training data\n",
    "        obj = TimeObject(df=data, column=pollutant, agg_freq='D')\n",
    "        obj.fixed_origin_rolling_window_cross_validation(\n",
    "            initial_train_date=None,\n",
    "            total_data_points=(len(obj.nixtla_df))-(k_fold*horizon),\n",
    "            h=horizon,\n",
    "            plot_interval=False\n",
    "        )\n",
    "\n",
    "        # Define the model\n",
    "        model = NHITS(\n",
    "            h=horizon,\n",
    "            input_size=input_size,\n",
    "            stack_types=n_stacks*['identity'],\n",
    "            n_freq_downsample=n_freq_downsample+(n_stacks-len(n_freq_downsample))*[1],\n",
    "            n_blocks=n_stacks*[n_blocks],\n",
    "            n_pool_kernel_size=(n_stacks-len(n_pool_kernel_size))*[1]+n_pool_kernel_size,\n",
    "            pooling_mode=\"MaxPool1d\",\n",
    "            activation=\"ReLU\",\n",
    "            interpolation_mode='linear',\n",
    "            max_steps=max_steps,\n",
    "            val_check_steps=10,\n",
    "            early_stop_patience_steps=int(np.round(max_steps/(20),0)),\n",
    "        )\n",
    "\n",
    "        # Initialize NeuralForecast and fit the model\n",
    "        fcst = NeuralForecast(\n",
    "            models=[model],\n",
    "            freq='D',\n",
    "            local_scaler_type=local_scalar_type\n",
    "        )\n",
    "        fcst.fit(df=obj.Y_train, verbose=False, val_size=horizon+1)\n",
    "        prediction = fcst.predict(df=obj.Y_train, verbose=False)\n",
    "\n",
    "        # Evaluate metrics\n",
    "        obj.metrics_(forecast_df=prediction, method='NHITS')\n",
    "        mape.append(obj.metrics['mape'])\n",
    "        smape.append(obj.metrics['smape'])\n",
    "        max.append(obj.metrics['max'])\n",
    "        mae.append(obj.metrics['mae'])\n",
    "        mse.append(obj.metrics['mse'])\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "\n",
    "    try:\n",
    "        directory_path = \"lightning_logs\"\n",
    "        if os.path.exists(directory_path):\n",
    "            shutil.rmtree(directory_path)\n",
    "    except:\n",
    "        ...\n",
    "\n",
    "    mape = np.mean(mape)\n",
    "    smape = np.mean(smape)\n",
    "    max = np.mean(max)\n",
    "    mae = np.mean(mae)\n",
    "    mse = np.mean(mse)\n",
    "\n",
    "    # Collect the results\n",
    "    results.append({\n",
    "        'poll': pollutant,\n",
    "        'freq': 'D',\n",
    "        'fold': max_k_fold,\n",
    "        'train_len': f'{(len(obj.nixtla_df))-(max_k_fold*horizon)} - {(len(obj.nixtla_df))-(0*horizon)}',\n",
    "        'h': horizon,\n",
    "        'input_size': input_size,\n",
    "        'n_stacks': n_stacks,\n",
    "        'n_blocks': n_blocks,\n",
    "        'max_steps': max_steps,\n",
    "        'local_scalar_type': local_scalar_type,\n",
    "        'n_pool_kernel_size': n_pool_kernel_size,\n",
    "        'n_freq_downsample': n_freq_downsample,\n",
    "        'mape': mape,\n",
    "        'smape': smape,\n",
    "        'max': max,\n",
    "        'mae': mae,\n",
    "        'mse': mse,\n",
    "    })\n",
    "\n",
    "    # The objective for Optuna is to minimize the MAE (or maximize a metric)\n",
    "    return smape, mae  # Any metric you want to optimize\n",
    "\n",
    "for pollutant in data[['pm10']]:\n",
    "    for h in [90]:\n",
    "        # Initialize the results list\n",
    "        results = []\n",
    "        # Define the optimization study_nhits\n",
    "        study_nhits = optuna.create_study(directions=['minimize','minimize'])  # Minimize the MAE\n",
    "\n",
    "        # Run the optimization with the number of trials you want\n",
    "        study_nhits.optimize(partial(objective, pollutant=pollutant, horizon=h), n_trials=200)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        NHITS_W = pd.DataFrame(results)\n",
    "\n",
    "        output_dir = fr'Results COBEQ\\NHITS (D)\\{pollutant}'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        NHITS_W.to_pickle(fr'Results COBEQ\\NHITS (D)\\{pollutant}\\{h}D_Df.pkl')\n",
    "        joblib.dump(study_nhits, fr\"Results COBEQ\\NHITS (D)\\{pollutant}\\{h}D_Study.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NBEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-13 09:47:10,279] Trial 199 finished with values: [39.852276, 9.647514000000001] and parameters: {'input_size': 925, 'n_stacks': 2, 'n_blocks': 2, 'max_steps': 494, 'local_scalar_type': 'boxcox', 'interpretability': ['trend', 'identity']}.\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function\n",
    "def objective(trial, pollutant, horizon):\n",
    "    # Hyperparameter search space\n",
    "    input_size = trial.suggest_int('input_size', 90, 1100, step=1)\n",
    "    n_stacks = trial.suggest_int('n_stacks', 2, 7, step=1)\n",
    "    n_blocks = trial.suggest_int('n_blocks', 1, 5, step=1)\n",
    "    max_steps = trial.suggest_int('max_steps', 10, 700, step=1)\n",
    "    local_scalar_type = trial.suggest_categorical('local_scalar_type', [None, 'standard', 'boxcox', 'minmax'])\n",
    "    interpretability = trial.suggest_categorical('interpretability', [list(combination) for combination in list(itertools.product(['seasonality', 'trend', 'identity'], repeat=2))])\n",
    "\n",
    "    mape = []\n",
    "    smape = []\n",
    "    max = []\n",
    "    mae = []\n",
    "    mse = []\n",
    "    # Split for cross validation\n",
    "    max_k_fold = 5\n",
    "    for k_fold in range(0,max_k_fold):\n",
    "        print(f'\\nPollutant = {pollutant} \\nh = {horizon} \\nTrial = {trial.number+1}\\nFold = {k_fold+1}\\n')\n",
    "        # Instantiate TimeObject and prepare training data\n",
    "        obj = TimeObject(df=data, column=pollutant, agg_freq='D')\n",
    "        obj.fixed_origin_rolling_window_cross_validation(\n",
    "            initial_train_date=None,\n",
    "            total_data_points=(len(obj.nixtla_df))-(k_fold*horizon),\n",
    "            h=horizon,\n",
    "            plot_interval=False\n",
    "        )\n",
    "\n",
    "        # Define the model\n",
    "        model = NBEATS(\n",
    "            h=horizon,\n",
    "            input_size=input_size,\n",
    "            stack_types=interpretability+(n_stacks-len(interpretability))*['identity'],\n",
    "            n_blocks=n_stacks * [n_blocks],\n",
    "            max_steps=max_steps,\n",
    "            learning_rate=1e-3,\n",
    "            val_check_steps=10,\n",
    "            early_stop_patience_steps=int(np.round(max_steps/(20),0)),\n",
    "        )\n",
    "\n",
    "        # Initialize NeuralForecast and fit the model\n",
    "        fcst = NeuralForecast(\n",
    "            models=[model],\n",
    "            freq='D',\n",
    "            local_scaler_type=local_scalar_type\n",
    "        )\n",
    "        fcst.fit(df=obj.Y_train, verbose=False, val_size=horizon+1)\n",
    "        prediction = fcst.predict(df=obj.Y_train, verbose=False)\n",
    "\n",
    "        # Evaluate metrics\n",
    "        obj.metrics_(forecast_df=prediction, method='NBEATS')\n",
    "        mape.append(obj.metrics['mape'])\n",
    "        smape.append(obj.metrics['smape'])\n",
    "        max.append(obj.metrics['max'])\n",
    "        mae.append(obj.metrics['mae'])\n",
    "        mse.append(obj.metrics['mse'])\n",
    "            \n",
    "        clear_output(wait=True)\n",
    "\n",
    "        try:\n",
    "            directory_path = \"lightning_logs\"\n",
    "            if os.path.exists(directory_path):\n",
    "                shutil.rmtree(directory_path)\n",
    "        except:\n",
    "            ...\n",
    "\n",
    "    mape = np.mean(mape)\n",
    "    smape = np.mean(smape)\n",
    "    max = np.mean(max)\n",
    "    mae = np.mean(mae)\n",
    "    mse = np.mean(mse)\n",
    "\n",
    "    # Collect the results\n",
    "    results.append({\n",
    "        'pollutant': pollutant,\n",
    "        'freq': 'D',\n",
    "        'fold': max_k_fold,\n",
    "        'train_len': f'{(len(obj.nixtla_df))-(max_k_fold*horizon)} - {(len(obj.nixtla_df))-(0*horizon)}',\n",
    "        'h': horizon,\n",
    "        'input_size': input_size,\n",
    "        'n_stacks': n_stacks,\n",
    "        'n_blocks': n_blocks,\n",
    "        'max_steps': max_steps,\n",
    "        'local_scalar_type': local_scalar_type,\n",
    "        'interpretability': interpretability,\n",
    "        'mape': mape,\n",
    "        'smape': smape,\n",
    "        'max': max,\n",
    "        'mae': mae,\n",
    "        'mse': mse,\n",
    "    })\n",
    "\n",
    "    # The objective for Optuna is to minimize the MAE (or maximize a metric)\n",
    "    return smape, mae  # Any metric you want to optimize\n",
    "\n",
    "for pollutant in data[['pm10']]:\n",
    "    for h in [90]:\n",
    "        # Initialize the results list\n",
    "        results = []\n",
    "        # Define the optimization study_nbeats\n",
    "        study_nbeats = optuna.create_study(directions=['minimize','minimize'])  # Minimize the MAE\n",
    "\n",
    "        # Run the optimization with the number of trials you want\n",
    "        study_nbeats.optimize(partial(objective, pollutant=pollutant, horizon=h), n_trials=200)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        NBEATS_W = pd.DataFrame(results)\n",
    "\n",
    "        output_dir = fr'Results COBEQ\\NBEATS (D)\\{pollutant}'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        NBEATS_W.to_pickle(fr'Results COBEQ\\NBEATS (D)\\{pollutant}\\{h}D_Df.pkl')\n",
    "        joblib.dump(study_nbeats, fr\"Results COBEQ\\NBEATS (D)\\{pollutant}\\{h}D_Study.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Statistical**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA, AutoCES, AutoETS, AutoTheta\n",
    "\n",
    "for pollutant in ['pm10',]:\n",
    "    for h in [14, 30, 60, 90]: #  30, 60, 90, 120\n",
    "        results_stats = pd.DataFrame()\n",
    "\n",
    "        mape = []\n",
    "        smape = []\n",
    "        max = []\n",
    "        mae = []\n",
    "        mse = []\n",
    "\n",
    "        max_k_fold = 5\n",
    "        for k_fold in range(0,max_k_fold):\n",
    "            print(f'\\nPollutant = {pollutant} \\nh = {h}\\nFold = {k_fold+1}\\n')\n",
    "            # Instantiate TimeObject and prepare training data\n",
    "            obj = TimeObject(df=data, column=pollutant, agg_freq='D')\n",
    "            obj.fixed_origin_rolling_window_cross_validation(\n",
    "                initial_train_date=None,\n",
    "                total_data_points=(len(obj.nixtla_df))-(k_fold*h),\n",
    "                h=h,\n",
    "                plot_interval=False\n",
    "            )\n",
    "\n",
    "            season_length = 365 # Monthly data \n",
    "\n",
    "            models = [\n",
    "                # AutoARIMA(season_length=season_length, alias='AutoARIMA'),\n",
    "                AutoCES(season_length=season_length, model='Z', alias='AutoCES-Z'),\n",
    "                AutoCES(season_length=season_length, model='S', alias='AutoCES-S'),\n",
    "                AutoCES(season_length=season_length, model='P', alias='AutoCES-P'),\n",
    "                AutoCES(season_length=season_length, model='N', alias='AutoCES-N'),\n",
    "                AutoTheta(season_length=season_length, decomposition_type=\"multiplicative\", alias='AutoTheta-Multi'),\n",
    "                AutoTheta(season_length=season_length, decomposition_type=\"additive\", alias='AutoTheta-Add'),\n",
    "            ]\n",
    "            models = models + [\n",
    "                AutoETS(season_length=season_length, model=ets, alias=f'AutoETS-{ets}')\n",
    "                for ets in ['ZAZ', 'ZAN', 'ZAA', 'ZAM', 'ZNN']]\n",
    "\n",
    "            frct = StatsForecast(models=models, freq='D')\n",
    "            frct.fit(df=obj.Y_train)\n",
    "            predicted = frct.predict(h=h)\n",
    "\n",
    "            columns = predicted.columns\n",
    "            columns = columns[(columns != 'ds') & (columns != 'unique_id')]\n",
    "\n",
    "            for method in columns:\n",
    "                obj.metrics_(predicted, method=method)\n",
    "                results_stats = pd.concat([results_stats, pd.DataFrame({\n",
    "                    'pollutant': [pollutant],\n",
    "                    'method': [method],\n",
    "                    'freq': ['D'],\n",
    "                    'h': [h],\n",
    "                    'mape': [obj.metrics['mape']],\n",
    "                    'smape': [obj.metrics['smape']],\n",
    "                    'max': [obj.metrics['max']],\n",
    "                    'mae': [obj.metrics['mae']],\n",
    "                    'mse': [obj.metrics['mse']]\n",
    "                })])\n",
    "            \n",
    "            # ======================================================================================================\n",
    "\n",
    "            nbeats = joblib.load(fr\"Results COBEQ\\NBEATS (D)\\{pollutant}\\{h}D_Study.pkl\")\n",
    "            model = NBEATS(\n",
    "                h=h,\n",
    "                input_size=nbeats.best_trials[0].params.get('input_size'),\n",
    "                stack_types=nbeats.best_trials[0].params.get('interpretability')+(nbeats.best_trials[0].params.get('n_stacks')-len(nbeats.best_trials[0].params.get('interpretability')))*['identity'],\n",
    "                n_blocks=nbeats.best_trials[0].params.get('n_stacks') * [nbeats.best_trials[0].params.get('n_blocks')],\n",
    "                max_steps=nbeats.best_trials[0].params.get('max_steps'),\n",
    "                learning_rate=1e-3,\n",
    "                val_check_steps=10,\n",
    "            )\n",
    "            fcst = NeuralForecast(\n",
    "                models=[model],\n",
    "                freq='D',\n",
    "                local_scaler_type=nbeats.best_trials[0].params.get('local_scalar_type')\n",
    "            )\n",
    "            fcst.fit(df=obj.Y_train, verbose=False)\n",
    "            predicted = fcst.predict(df=obj.Y_train, verbose=False)\n",
    "            obj.metrics_(predicted, method='NBEATS')\n",
    "            results_stats = pd.concat([results_stats, pd.DataFrame({\n",
    "                'pollutant': [pollutant],\n",
    "                'method': ['NBEATS'],\n",
    "                'freq': ['D'],\n",
    "                'h': [h],\n",
    "                'mape': [obj.metrics['mape']],\n",
    "                'smape': [obj.metrics['smape']],\n",
    "                'max': [obj.metrics['max']],\n",
    "                'mae': [obj.metrics['mae']],\n",
    "                'mse': [obj.metrics['mse']]\n",
    "            })])\n",
    "\n",
    "            # ======================================================================================================\n",
    "            \n",
    "            nhits = joblib.load(fr\"Results COBEQ\\NHITS (D)\\{pollutant}\\{h}D_Study.pkl\")\n",
    "            model = NHITS(\n",
    "                h=h,\n",
    "                input_size=nhits.best_trials[0].params.get('input_size'),\n",
    "                stack_types=nhits.best_trials[0].params.get('n_stacks')*['identity'],\n",
    "                n_freq_downsample=nhits.best_trials[0].params.get('n_freq_downsample')+(nhits.best_trials[0].params.get('n_stacks')-len(nhits.best_trials[0].params.get('n_freq_downsample')))*[1],\n",
    "                n_blocks=nhits.best_trials[0].params.get('n_stacks')*[nhits.best_trials[0].params.get('n_blocks')],\n",
    "                n_pool_kernel_size=(nhits.best_trials[0].params.get('n_stacks')-len(nhits.best_trials[0].params.get('n_pool_kernel_size')))*[1]+nhits.best_trials[0].params.get('n_pool_kernel_size'),\n",
    "                pooling_mode=\"MaxPool1d\",\n",
    "                activation=\"ReLU\",\n",
    "                interpolation_mode='linear',\n",
    "                max_steps=nhits.best_trials[0].params.get('max_steps'),\n",
    "                val_check_steps=10,\n",
    "            )\n",
    "            fcst = NeuralForecast(\n",
    "                models=[model],\n",
    "                freq='D',\n",
    "                local_scaler_type=nhits.best_trials[0].params.get('local_scalar_type')\n",
    "            )\n",
    "            fcst.fit(df=obj.Y_train, verbose=False)\n",
    "            predicted = fcst.predict(df=obj.Y_train, verbose=False)\n",
    "            obj.metrics_(predicted, method='NHITS')\n",
    "            results_stats = pd.concat([results_stats, pd.DataFrame({\n",
    "                'pollutant': [pollutant],\n",
    "                'method': ['NHITS'],\n",
    "                'freq': ['D'],\n",
    "                'h': [h],\n",
    "                'mape': [obj.metrics['mape']],\n",
    "                'smape': [obj.metrics['smape']],\n",
    "                'max': [obj.metrics['max']],\n",
    "                'mae': [obj.metrics['mae']],\n",
    "                'mse': [obj.metrics['mse']]\n",
    "            })])\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "\n",
    "        # ======================================================================================================\n",
    "\n",
    "        results_stats = pd.DataFrame(results_stats)\n",
    "\n",
    "        output_dir = fr'Results COBEQ\\Stats (D)\\{pollutant}'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        results_stats.to_pickle(fr'Results COBEQ\\Stats (D)\\{pollutant}\\{h}D_Df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "pollutant",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "method",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "h",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "freq",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mape",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "smape",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "max",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mae",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mse",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "731bb369-fcf0-474b-bbc4-3b994c0e9da7",
       "rows": [
        [
         "11",
         "pm10",
         "NBEATS",
         "14",
         "D",
         "44.98025",
         "30.756068",
         "14.409683999999999",
         "6.24041",
         "58.259631999999996"
        ],
        [
         "4",
         "pm10",
         "AutoETS-ZAA",
         "14",
         "D",
         "45.389148",
         "32.218574000000004",
         "16.316548",
         "6.449181999999999",
         "63.30363200000001"
        ],
        [
         "5",
         "pm10",
         "AutoETS-ZAM",
         "14",
         "D",
         "48.698972",
         "32.922966",
         "16.56821",
         "6.6780740000000005",
         "66.048838"
        ],
        [
         "3",
         "pm10",
         "AutoCES-Z",
         "14",
         "D",
         "46.569904",
         "33.080242",
         "17.072589999999998",
         "6.668438",
         "66.93023"
        ],
        [
         "12",
         "pm10",
         "NHITS",
         "14",
         "D",
         "47.513166",
         "33.135968000000005",
         "15.277778000000001",
         "6.513646",
         "65.666558"
        ],
        [
         "2",
         "pm10",
         "AutoCES-S",
         "14",
         "D",
         "51.427136",
         "34.27067",
         "16.822682",
         "7.0426079999999995",
         "70.399416"
        ],
        [
         "9",
         "pm10",
         "AutoTheta-Add",
         "14",
         "D",
         "54.295262",
         "36.573534",
         "17.344894",
         "7.509022",
         "92.038578"
        ],
        [
         "8",
         "pm10",
         "AutoETS-ZNN",
         "14",
         "D",
         "55.560676",
         "39.680057999999995",
         "15.959988",
         "7.842228",
         "88.494788"
        ],
        [
         "10",
         "pm10",
         "AutoTheta-Multi",
         "14",
         "D",
         "61.288419999999995",
         "39.86123",
         "19.165976",
         "8.75562",
         "130.427658"
        ],
        [
         "0",
         "pm10",
         "AutoCES-N",
         "14",
         "D",
         "44.178154",
         "46.94581",
         "18.76294",
         "8.372741999999999",
         "110.929892"
        ],
        [
         "1",
         "pm10",
         "AutoCES-P",
         "14",
         "D",
         "47.574016",
         "51.045201999999996",
         "19.47506",
         "8.693818",
         "110.17726"
        ],
        [
         "7",
         "pm10",
         "AutoETS-ZAZ",
         "14",
         "D",
         "118.25203400000001",
         "56.033176000000005",
         "29.77265",
         "15.671949999999999",
         "341.98184200000003"
        ],
        [
         "6",
         "pm10",
         "AutoETS-ZAN",
         "14",
         "D",
         "118.269528",
         "56.041782",
         "29.775162",
         "15.67605",
         "342.14349400000003"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 13
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pollutant</th>\n",
       "      <th>method</th>\n",
       "      <th>h</th>\n",
       "      <th>freq</th>\n",
       "      <th>mape</th>\n",
       "      <th>smape</th>\n",
       "      <th>max</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pm10</td>\n",
       "      <td>NBEATS</td>\n",
       "      <td>14</td>\n",
       "      <td>D</td>\n",
       "      <td>44.980250</td>\n",
       "      <td>30.756068</td>\n",
       "      <td>14.409684</td>\n",
       "      <td>6.240410</td>\n",
       "      <td>58.259632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pm10</td>\n",
       "      <td>AutoETS-ZAA</td>\n",
       "      <td>14</td>\n",
       "      <td>D</td>\n",
       "      <td>45.389148</td>\n",
       "      <td>32.218574</td>\n",
       "      <td>16.316548</td>\n",
       "      <td>6.449182</td>\n",
       "      <td>63.303632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pm10</td>\n",
       "      <td>AutoETS-ZAM</td>\n",
       "      <td>14</td>\n",
       "      <td>D</td>\n",
       "      <td>48.698972</td>\n",
       "      <td>32.922966</td>\n",
       "      <td>16.568210</td>\n",
       "      <td>6.678074</td>\n",
       "      <td>66.048838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pm10</td>\n",
       "      <td>AutoCES-Z</td>\n",
       "      <td>14</td>\n",
       "      <td>D</td>\n",
       "      <td>46.569904</td>\n",
       "      <td>33.080242</td>\n",
       "      <td>17.072590</td>\n",
       "      <td>6.668438</td>\n",
       "      <td>66.930230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pm10</td>\n",
       "      <td>NHITS</td>\n",
       "      <td>14</td>\n",
       "      <td>D</td>\n",
       "      <td>47.513166</td>\n",
       "      <td>33.135968</td>\n",
       "      <td>15.277778</td>\n",
       "      <td>6.513646</td>\n",
       "      <td>65.666558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pm10</td>\n",
       "      <td>AutoCES-S</td>\n",
       "      <td>14</td>\n",
       "      <td>D</td>\n",
       "      <td>51.427136</td>\n",
       "      <td>34.270670</td>\n",
       "      <td>16.822682</td>\n",
       "      <td>7.042608</td>\n",
       "      <td>70.399416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pm10</td>\n",
       "      <td>AutoTheta-Add</td>\n",
       "      <td>14</td>\n",
       "      <td>D</td>\n",
       "      <td>54.295262</td>\n",
       "      <td>36.573534</td>\n",
       "      <td>17.344894</td>\n",
       "      <td>7.509022</td>\n",
       "      <td>92.038578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pm10</td>\n",
       "      <td>AutoETS-ZNN</td>\n",
       "      <td>14</td>\n",
       "      <td>D</td>\n",
       "      <td>55.560676</td>\n",
       "      <td>39.680058</td>\n",
       "      <td>15.959988</td>\n",
       "      <td>7.842228</td>\n",
       "      <td>88.494788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pm10</td>\n",
       "      <td>AutoTheta-Multi</td>\n",
       "      <td>14</td>\n",
       "      <td>D</td>\n",
       "      <td>61.288420</td>\n",
       "      <td>39.861230</td>\n",
       "      <td>19.165976</td>\n",
       "      <td>8.755620</td>\n",
       "      <td>130.427658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pm10</td>\n",
       "      <td>AutoCES-N</td>\n",
       "      <td>14</td>\n",
       "      <td>D</td>\n",
       "      <td>44.178154</td>\n",
       "      <td>46.945810</td>\n",
       "      <td>18.762940</td>\n",
       "      <td>8.372742</td>\n",
       "      <td>110.929892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pm10</td>\n",
       "      <td>AutoCES-P</td>\n",
       "      <td>14</td>\n",
       "      <td>D</td>\n",
       "      <td>47.574016</td>\n",
       "      <td>51.045202</td>\n",
       "      <td>19.475060</td>\n",
       "      <td>8.693818</td>\n",
       "      <td>110.177260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pm10</td>\n",
       "      <td>AutoETS-ZAZ</td>\n",
       "      <td>14</td>\n",
       "      <td>D</td>\n",
       "      <td>118.252034</td>\n",
       "      <td>56.033176</td>\n",
       "      <td>29.772650</td>\n",
       "      <td>15.671950</td>\n",
       "      <td>341.981842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pm10</td>\n",
       "      <td>AutoETS-ZAN</td>\n",
       "      <td>14</td>\n",
       "      <td>D</td>\n",
       "      <td>118.269528</td>\n",
       "      <td>56.041782</td>\n",
       "      <td>29.775162</td>\n",
       "      <td>15.676050</td>\n",
       "      <td>342.143494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pollutant           method   h freq        mape      smape        max  \\\n",
       "11      pm10           NBEATS  14    D   44.980250  30.756068  14.409684   \n",
       "4       pm10      AutoETS-ZAA  14    D   45.389148  32.218574  16.316548   \n",
       "5       pm10      AutoETS-ZAM  14    D   48.698972  32.922966  16.568210   \n",
       "3       pm10        AutoCES-Z  14    D   46.569904  33.080242  17.072590   \n",
       "12      pm10            NHITS  14    D   47.513166  33.135968  15.277778   \n",
       "2       pm10        AutoCES-S  14    D   51.427136  34.270670  16.822682   \n",
       "9       pm10    AutoTheta-Add  14    D   54.295262  36.573534  17.344894   \n",
       "8       pm10      AutoETS-ZNN  14    D   55.560676  39.680058  15.959988   \n",
       "10      pm10  AutoTheta-Multi  14    D   61.288420  39.861230  19.165976   \n",
       "0       pm10        AutoCES-N  14    D   44.178154  46.945810  18.762940   \n",
       "1       pm10        AutoCES-P  14    D   47.574016  51.045202  19.475060   \n",
       "7       pm10      AutoETS-ZAZ  14    D  118.252034  56.033176  29.772650   \n",
       "6       pm10      AutoETS-ZAN  14    D  118.269528  56.041782  29.775162   \n",
       "\n",
       "          mae         mse  \n",
       "11   6.240410   58.259632  \n",
       "4    6.449182   63.303632  \n",
       "5    6.678074   66.048838  \n",
       "3    6.668438   66.930230  \n",
       "12   6.513646   65.666558  \n",
       "2    7.042608   70.399416  \n",
       "9    7.509022   92.038578  \n",
       "8    7.842228   88.494788  \n",
       "10   8.755620  130.427658  \n",
       "0    8.372742  110.929892  \n",
       "1    8.693818  110.177260  \n",
       "7   15.671950  341.981842  \n",
       "6   15.676050  342.143494  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_stats = joblib.load(r'Results COBEQ\\Stats (D)\\pm10\\14D_Df.pkl')\n",
    "results = results_stats.groupby(['pollutant','method','h','freq'])[['mape', 'smape', 'max', 'mae', 'mse']].mean().reset_index().sort_values('smape')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for model in ['NHITS','NBEATS']:\n",
    "    for H in [12,26,52,78]:\n",
    "        x = joblib.load(fr'Results COBEQ/Stats (W)/pm10/{H}W_Df.pkl').sort_values(['smape','mae']).reset_index()\n",
    "        metrics = ['smape', 'mae', 'mape', 'mse', 'max']\n",
    "        for metric in metrics:\n",
    "            x[f'{metric}_rank'] = x[metric].rank(method='min')\n",
    "\n",
    "        # If you want best = 1, worst = 21, and ranks to be integers\n",
    "        x[[f'{m}_rank' for m in metrics]] = x[[f'{m}_rank' for m in metrics]].astype(int)\n",
    "        df = pd.concat([df, x[x['method'] == model]])\n",
    "        del df['index']\n",
    "df.to_excel(r'x.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for model in ['NHITS','NBEATS']:\n",
    "    for H in [12,26,52,78]:\n",
    "        x = joblib.load(fr'C:\\Users\\gustavo.filho\\Documents\\Python\\GitHub\\TCC\\Results COBEQ\\{model} (W)\\pm10\\{H}W_Df.pkl').sort_values(['smape','mae']).reset_index()\n",
    "        df = pd.concat([df, pd.DataFrame(x.iloc[0,:]).T])\n",
    "\n",
    "df.to_excel(r'x.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA, AutoCES, AutoETS, AutoTheta\n",
    "\n",
    "results_stats = []\n",
    "for pollutant in ['pm10']:\n",
    "    for h in [365]:\n",
    "\n",
    "        obj = TimeObject(data, pollutant, agg_freq='D')\n",
    "        obj.NIXTLA_train_test(split=h)\n",
    "\n",
    "        season_length = 365 # Monthly data \n",
    "        horizon = len(obj.Y_train) # number of predictions\n",
    "\n",
    "        models = [\n",
    "            # AutoARIMA(season_length=season_length, alias='AutoARIMA'),\n",
    "            AutoCES(season_length=season_length, model='Z', alias='AutoCES-Z'),\n",
    "            AutoCES(season_length=season_length, model='S', alias='AutoCES-S'),\n",
    "            AutoCES(season_length=season_length, model='P', alias='AutoCES-P'),\n",
    "            AutoCES(season_length=season_length, model='N', alias='AutoCES-N'),\n",
    "            AutoTheta(season_length=season_length, decomposition_type=\"multiplicative\", alias='AutoTheta-Multi'),\n",
    "            AutoTheta(season_length=season_length, decomposition_type=\"additive\", alias='AutoTheta-Add'),\n",
    "        ]\n",
    "        models = models + [\n",
    "            AutoETS(season_length=season_length, model=ets, alias=f'AutoETS-{ets}')\n",
    "            for ets in [f\"{e}{t}{s}\" for e in ['Z',]\n",
    "                        for t in ['Z', 'A', 'N'] \n",
    "                        for s in ['Z', 'A', 'M', 'N'] \n",
    "        ]]\n",
    "\n",
    "        frct = StatsForecast(models=models, freq='D')\n",
    "        frct.fit(df=obj.Y_train)\n",
    "        predicted = frct.predict(h=h)\n",
    "\n",
    "        columns = predicted.columns\n",
    "        columns = columns[(columns != 'ds') & (columns != 'unique_id')]\n",
    "\n",
    "        results_stats = pd.DataFrame()\n",
    "        for method in columns:\n",
    "            obj.plot_forecast(predicted, show_metrics=True, method=method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pollutant in ['pm10']:\n",
    "    for h in [14, 30, 60, 90]:\n",
    "\n",
    "        obj = TimeObject(data, pollutant, agg_freq='D')\n",
    "        obj.NIXTLA_train_test(split=h)\n",
    "        horizon = len(obj.Y_train) # number of predictions\n",
    "\n",
    "        # ======================================================================================================\n",
    "\n",
    "        nbeats = joblib.load(fr\"Results COBEQ\\NBEATS (W)\\{pollutant}\\78W_Study.pkl\")\n",
    "        model = NBEATS(\n",
    "            h=h,\n",
    "            input_size=nbeats.best_trials[0].params.get('input_size'),\n",
    "            stack_types=nbeats.best_trials[0].params.get('interpretability')+(nbeats.best_trials[0].params.get('n_stacks')-len(nbeats.best_trials[0].params.get('interpretability')))*['identity'],\n",
    "            n_blocks=nbeats.best_trials[0].params.get('n_stacks') * [nbeats.best_trials[0].params.get('n_blocks')],\n",
    "            max_steps=nbeats.best_trials[0].params.get('max_steps'),\n",
    "            learning_rate=1e-3,\n",
    "            val_check_steps=10,\n",
    "        )\n",
    "        fcst = NeuralForecast(\n",
    "            models=[model],\n",
    "            freq='D',\n",
    "            local_scaler_type=nbeats.best_trials[0].params.get('local_scalar_type')\n",
    "        )\n",
    "        fcst.fit(df=obj.Y_train, verbose=False)\n",
    "        predicted = fcst.predict(df=obj.Y_train, verbose=False)\n",
    "        obj.plot_forecast(predicted, method='NBEATS', show_metrics=True)\n",
    "\n",
    "        # ======================================================================================================\n",
    "        \n",
    "        nhits = joblib.load(fr\"Results COBEQ\\NHITS (W)\\{pollutant}\\78W_Study.pkl\")\n",
    "        model = NHITS(\n",
    "            h=h,\n",
    "            input_size=nhits.best_trials[0].params.get('input_size'),\n",
    "            stack_types=nhits.best_trials[0].params.get('n_stacks')*['identity'],\n",
    "            n_freq_downsample=nhits.best_trials[0].params.get('n_freq_downsample')+(nhits.best_trials[0].params.get('n_stacks')-len(nhits.best_trials[0].params.get('n_freq_downsample')))*[1],\n",
    "            n_blocks=nhits.best_trials[0].params.get('n_stacks')*[nhits.best_trials[0].params.get('n_blocks')],\n",
    "            n_pool_kernel_size=(nhits.best_trials[0].params.get('n_stacks')-len(nhits.best_trials[0].params.get('n_pool_kernel_size')))*[1]+nhits.best_trials[0].params.get('n_pool_kernel_size'),\n",
    "            pooling_mode=\"MaxPool1d\",\n",
    "            activation=\"ReLU\",\n",
    "            interpolation_mode='linear',\n",
    "            max_steps=nhits.best_trials[0].params.get('max_steps'),\n",
    "            val_check_steps=10,\n",
    "        )\n",
    "        fcst = NeuralForecast(\n",
    "            models=[model],\n",
    "            freq='D',\n",
    "            local_scaler_type=nhits.best_trials[0].params.get('local_scalar_type')\n",
    "        )\n",
    "        fcst.fit(df=obj.Y_train, verbose=False)\n",
    "        predicted = fcst.predict(df=obj.Y_train, verbose=False)\n",
    "        obj.plot_forecast(predicted, show_metrics=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
