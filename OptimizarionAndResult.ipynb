{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **COBEQ 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "import optuna\n",
    "import itertools\n",
    "import shutil\n",
    "from functools import partial\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.subplots\n",
    "import time\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NBEATS, NHITS\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "ponte = pd.read_pickle(r'Data\\Data_Ponte_dos_Remedios.pkl')\n",
    "del ponte['o3']\n",
    "guarulhos = pd.read_pickle(r'Data\\Data_Guarulhos.pkl')\n",
    "guarulhos = guarulhos[['date','o3']]\n",
    "\n",
    "data = ponte.merge(guarulhos, on='date', how='outer')\n",
    "data.reset_index(drop=True)\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "os.environ['NIXTLA_ID_AS_COL'] = '1'\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "trainer = Trainer(\n",
    "    max_steps=4,\n",
    "    logger=False,\n",
    "    enable_progress_bar=False,\n",
    "    enable_model_summary=False  # Disable model summary\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"optuna\")\n",
    "\n",
    "from TimeObjectModule import TimeObject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Experimental Planning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in [14, 30, 60, 90, 120]:\n",
    "    for k in [1, 2, 3, 4, 5,]:\n",
    "        x = TimeObject(data, column='pm10', agg_freq='D')\n",
    "        x.fixed_origin_rolling_window_cross_validation(\n",
    "            initial_train_date=0,\n",
    "            total_data_points=1000+(k*h),\n",
    "            h=h,\n",
    "            # plot_interval=True,\n",
    "            width=1100,height=400\n",
    "        )\n",
    "        # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H = 14\n",
      "Fold: 1 | Train = 2452 | Percent = 99.432\n",
      "Fold: 2 | Train = 2438 | Percent = 98.865\n",
      "Fold: 3 | Train = 2424 | Percent = 98.297\n",
      "Fold: 4 | Train = 2410 | Percent = 97.729\n",
      "Fold: 5 | Train = 2396 | Percent = 97.161\n",
      "=================\n",
      "Fold: 1 | Train = 2382 | Percent = 96.594\n",
      "Fold: 2 | Train = 2368 | Percent = 96.026\n",
      "Fold: 3 | Train = 2354 | Percent = 95.458\n",
      "Fold: 4 | Train = 2340 | Percent = 94.891\n",
      "Fold: 5 | Train = 2326 | Percent = 94.323\n",
      "\n",
      "\n",
      "H = 30\n",
      "Fold: 1 | Train = 2436 | Percent = 98.783\n",
      "Fold: 2 | Train = 2406 | Percent = 97.567\n",
      "Fold: 3 | Train = 2376 | Percent = 96.35\n",
      "Fold: 4 | Train = 2346 | Percent = 95.134\n",
      "Fold: 5 | Train = 2316 | Percent = 93.917\n",
      "=================\n",
      "Fold: 1 | Train = 2286 | Percent = 92.701\n",
      "Fold: 2 | Train = 2256 | Percent = 91.484\n",
      "Fold: 3 | Train = 2226 | Percent = 90.268\n",
      "Fold: 4 | Train = 2196 | Percent = 89.051\n",
      "Fold: 5 | Train = 2166 | Percent = 87.835\n",
      "\n",
      "\n",
      "H = 60\n",
      "Fold: 1 | Train = 2406 | Percent = 97.567\n",
      "Fold: 2 | Train = 2346 | Percent = 95.134\n",
      "Fold: 3 | Train = 2286 | Percent = 92.701\n",
      "Fold: 4 | Train = 2226 | Percent = 90.268\n",
      "Fold: 5 | Train = 2166 | Percent = 87.835\n",
      "=================\n",
      "Fold: 1 | Train = 2106 | Percent = 85.401\n",
      "Fold: 2 | Train = 2046 | Percent = 82.968\n",
      "Fold: 3 | Train = 1986 | Percent = 80.535\n",
      "Fold: 4 | Train = 1926 | Percent = 78.102\n",
      "Fold: 5 | Train = 1866 | Percent = 75.669\n",
      "\n",
      "\n",
      "H = 90\n",
      "Fold: 1 | Train = 2376 | Percent = 96.35\n",
      "Fold: 2 | Train = 2286 | Percent = 92.701\n",
      "Fold: 3 | Train = 2196 | Percent = 89.051\n",
      "Fold: 4 | Train = 2106 | Percent = 85.401\n",
      "Fold: 5 | Train = 2016 | Percent = 81.752\n",
      "=================\n",
      "Fold: 1 | Train = 1926 | Percent = 78.102\n",
      "Fold: 2 | Train = 1836 | Percent = 74.453\n",
      "Fold: 3 | Train = 1746 | Percent = 70.803\n",
      "Fold: 4 | Train = 1656 | Percent = 67.153\n",
      "Fold: 5 | Train = 1566 | Percent = 63.504\n",
      "\n",
      "\n",
      "H = 120\n",
      "Fold: 1 | Train = 2346 | Percent = 95.134\n",
      "Fold: 2 | Train = 2226 | Percent = 90.268\n",
      "Fold: 3 | Train = 2106 | Percent = 85.401\n",
      "Fold: 4 | Train = 1986 | Percent = 80.535\n",
      "Fold: 5 | Train = 1866 | Percent = 75.669\n",
      "=================\n",
      "Fold: 1 | Train = 1746 | Percent = 70.803\n",
      "Fold: 2 | Train = 1626 | Percent = 65.937\n",
      "Fold: 3 | Train = 1506 | Percent = 61.071\n",
      "Fold: 4 | Train = 1386 | Percent = 56.204\n",
      "Fold: 5 | Train = 1266 | Percent = 51.338\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for horizon in [14, 30, 60, 90, 120]:\n",
    "    print(f'H = {horizon}')\n",
    "    max_k_fold = 5\n",
    "    for k_fold in range(0,max_k_fold):\n",
    "        obj = TimeObject(df=data, column='pm10', agg_freq='D')\n",
    "        obj.fixed_origin_rolling_window_cross_validation(\n",
    "            initial_train_date=None,\n",
    "            total_data_points=(len(obj.nixtla_df))-(k_fold*horizon),\n",
    "            h=horizon,\n",
    "            # plot_interval=True\n",
    "        )\n",
    "        print(f'Fold: {k_fold+1} | Train = {len(obj.Y_train)} | Percent = {round(100*len(obj.Y_train)/len(obj.nixtla_df),3)}')\n",
    "    print('=================')\n",
    "    for k_fold in range(0,max_k_fold):\n",
    "        obj = TimeObject(df=data, column='pm10', agg_freq='D')\n",
    "        obj.fixed_origin_rolling_window_cross_validation(\n",
    "            initial_train_date=None,\n",
    "            total_data_points=(len(obj.nixtla_df)-(max_k_fold*horizon))-(k_fold*horizon),\n",
    "            h=horizon,\n",
    "            # plot_interval=True\n",
    "        )\n",
    "        print(f'Fold: {k_fold+1} | Train = {len(obj.Y_train)} | Percent = {round(100*len(obj.Y_train)/len(obj.nixtla_df),3)}')\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Optuna**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-HiTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective(trial, pollutant, horizon):\n",
    "    # Hyperparameter search space\n",
    "    input_size = trial.suggest_int('input_size', 90, 1100, step=1)\n",
    "    n_stacks = trial.suggest_int('n_stacks', 3, 7, step=1)\n",
    "    n_blocks = trial.suggest_int('n_blocks', 1, 7, step=1)\n",
    "    max_steps = trial.suggest_int('max_steps', 10, 700, step=1)\n",
    "    local_scalar_type = trial.suggest_categorical('local_scalar_type', [None, 'standard', 'boxcox', 'minmax'])\n",
    "    n_pool_kernel_size = trial.suggest_categorical('n_pool_kernel_size', [list(combination) for combination in list(itertools.product([1, 2, 3], repeat=3))])\n",
    "    n_freq_downsample = trial.suggest_categorical('n_freq_downsample', [list(combination) for combination in list(itertools.product([1, 4, 12, 52], repeat=3))])\n",
    "\n",
    "    mape = []\n",
    "    smape = []\n",
    "    max = []\n",
    "    mae = []\n",
    "    mse = []\n",
    "    # Split for cross validation\n",
    "    max_k_fold = 5\n",
    "    for k_fold in range(0,max_k_fold):\n",
    "        print(f'\\nPollutant = {pollutant} \\nh = {horizon} \\nTrial = {trial.number+1}\\nFold = {k_fold+1}\\n')\n",
    "        # Instantiate TimeObject and prepare training data\n",
    "        obj = TimeObject(df=data, column=pollutant, agg_freq='D')\n",
    "        obj.fixed_origin_rolling_window_cross_validation(\n",
    "            initial_train_date=None,\n",
    "            total_data_points=(len(obj.nixtla_df))-(k_fold*horizon),\n",
    "            h=horizon,\n",
    "            plot_interval=True\n",
    "        )\n",
    "\n",
    "        # Define the model\n",
    "        model = NHITS(\n",
    "            h=horizon,\n",
    "            input_size=input_size,\n",
    "            stack_types=n_stacks*['identity'],\n",
    "            n_freq_downsample=n_freq_downsample+(n_stacks-len(n_freq_downsample))*[1],\n",
    "            n_blocks=n_stacks*[n_blocks],\n",
    "            n_pool_kernel_size=(n_stacks-len(n_pool_kernel_size))*[1]+n_pool_kernel_size,\n",
    "            pooling_mode=\"MaxPool1d\",\n",
    "            activation=\"ReLU\",\n",
    "            interpolation_mode='linear',\n",
    "            max_steps=max_steps,\n",
    "            val_check_steps=10,\n",
    "            early_stop_patience_steps=int(np.round(max_steps/(20),0)),\n",
    "        )\n",
    "\n",
    "        # Initialize NeuralForecast and fit the model\n",
    "        fcst = NeuralForecast(\n",
    "            models=[model],\n",
    "            freq='D',\n",
    "            local_scaler_type=local_scalar_type\n",
    "        )\n",
    "        fcst.fit(df=obj.Y_train, verbose=False, val_size=horizon+1)\n",
    "        prediction = fcst.predict(df=obj.Y_train, verbose=False)\n",
    "\n",
    "        # Evaluate metrics\n",
    "        obj.metrics_(forecast_df=prediction, method='NHITS')\n",
    "        mape.append(obj.metrics['mape'])\n",
    "        smape.append(obj.metrics['smape'])\n",
    "        max.append(obj.metrics['max'])\n",
    "        mae.append(obj.metrics['mae'])\n",
    "        mse.append(obj.metrics['mse'])\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "\n",
    "    try:\n",
    "        directory_path = \"lightning_logs\"\n",
    "        if os.path.exists(directory_path):\n",
    "            shutil.rmtree(directory_path)\n",
    "    except:\n",
    "        ...\n",
    "\n",
    "    mape = np.mean(mape)\n",
    "    smape = np.mean(smape)\n",
    "    max = np.mean(max)\n",
    "    mae = np.mean(mae)\n",
    "    mse = np.mean(mse)\n",
    "\n",
    "    # Collect the results\n",
    "    results.append({\n",
    "        'poll': pollutant,\n",
    "        'freq': 'D',\n",
    "        'fold': max_k_fold,\n",
    "        'train_len': f'{(len(obj.nixtla_df))-(max_k_fold*horizon)} - {(len(obj.nixtla_df))-(0*horizon)}',\n",
    "        'h': horizon,\n",
    "        'input_size': input_size,\n",
    "        'n_stacks': n_stacks,\n",
    "        'n_blocks': n_blocks,\n",
    "        'max_steps': max_steps,\n",
    "        'local_scalar_type': local_scalar_type,\n",
    "        'n_pool_kernel_size': n_pool_kernel_size,\n",
    "        'n_freq_downsample': n_freq_downsample,\n",
    "        'mape': mape,\n",
    "        'smape': smape,\n",
    "        'max': max,\n",
    "        'mae': mae,\n",
    "        'mse': mse,\n",
    "    })\n",
    "\n",
    "    # The objective for Optuna is to minimize the MAE (or maximize a metric)\n",
    "    return smape, mae  # Any metric you want to optimize\n",
    "\n",
    "for pollutant in data[['pm10']]:\n",
    "    for h in [14, 30, 60, 90, 120]:\n",
    "        # Initialize the results list\n",
    "        results = []\n",
    "        # Define the optimization study_nhits\n",
    "        study_nhits = optuna.create_study(directions=['minimize','minimize'])  # Minimize the MAE\n",
    "\n",
    "        # Run the optimization with the number of trials you want\n",
    "        study_nhits.optimize(partial(objective, pollutant=pollutant, horizon=h), n_trials=3)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        NHITS_W = pd.DataFrame(results)\n",
    "\n",
    "        output_dir = fr'Results COBEQ\\NHITS (D)\\{pollutant}'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        NHITS_W.to_pickle(fr'Results COBEQ\\NHITS (D)\\{pollutant}\\{h}D_Df.pkl')\n",
    "        joblib.dump(study_nhits, fr\"Results COBEQ\\NHITS (D)\\{pollutant}\\{h}D_Study.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NBEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective(trial, pollutant, horizon):\n",
    "    # Hyperparameter search space\n",
    "    input_size = trial.suggest_int('input_size', 90, 1100, step=1)\n",
    "    n_stacks = trial.suggest_int('n_stacks', 2, 7, step=1)\n",
    "    n_blocks = trial.suggest_int('n_blocks', 1, 5, step=1)\n",
    "    max_steps = trial.suggest_int('max_steps', 10, 700, step=1)\n",
    "    local_scalar_type = trial.suggest_categorical('local_scalar_type', [None, 'standard', 'boxcox', 'minmax'])\n",
    "    interpretability = trial.suggest_categorical('interpretability', [list(combination) for combination in list(itertools.product(['seasonality', 'trend', 'identity'], repeat=2))])\n",
    "\n",
    "    mape = []\n",
    "    smape = []\n",
    "    max = []\n",
    "    mae = []\n",
    "    mse = []\n",
    "    # Split for cross validation\n",
    "    max_k_fold = 5\n",
    "    for k_fold in range(0,max_k_fold):\n",
    "        print(f'\\nPollutant = {pollutant} \\nh = {horizon} \\nTrial = {trial.number+1}\\nFold = {k_fold+1}\\n')\n",
    "        # Instantiate TimeObject and prepare training data\n",
    "        obj = TimeObject(df=data, column=pollutant, agg_freq='D')\n",
    "        obj.fixed_origin_rolling_window_cross_validation(\n",
    "            initial_train_date=None,\n",
    "            total_data_points=(len(obj.nixtla_df))-(k_fold*horizon),\n",
    "            h=horizon,\n",
    "            plot_interval=True\n",
    "        )\n",
    "\n",
    "        # Define the model\n",
    "        model = NBEATS(\n",
    "            h=horizon,\n",
    "            input_size=input_size,\n",
    "            stack_types=interpretability+(n_stacks-len(interpretability))*['identity'],\n",
    "            n_blocks=n_stacks * [n_blocks],\n",
    "            max_steps=max_steps,\n",
    "            learning_rate=1e-3,\n",
    "            val_check_steps=10,\n",
    "            early_stop_patience_steps=int(np.round(max_steps/(20),0)),\n",
    "        )\n",
    "\n",
    "        # Initialize NeuralForecast and fit the model\n",
    "        fcst = NeuralForecast(\n",
    "            models=[model],\n",
    "            freq='D',\n",
    "            local_scaler_type=local_scalar_type\n",
    "        )\n",
    "        fcst.fit(df=obj.Y_train, verbose=False, val_size=horizon+1)\n",
    "        prediction = fcst.predict(df=obj.Y_train, verbose=False)\n",
    "\n",
    "        # Evaluate metrics\n",
    "        obj.metrics_(forecast_df=prediction, method='NBEATS')\n",
    "        mape.append(obj.metrics['mape'])\n",
    "        smape.append(obj.metrics['smape'])\n",
    "        max.append(obj.metrics['max'])\n",
    "        mae.append(obj.metrics['mae'])\n",
    "        mse.append(obj.metrics['mse'])\n",
    "            \n",
    "        clear_output(wait=True)\n",
    "\n",
    "        try:\n",
    "            directory_path = \"lightning_logs\"\n",
    "            if os.path.exists(directory_path):\n",
    "                shutil.rmtree(directory_path)\n",
    "        except:\n",
    "            ...\n",
    "\n",
    "    mape = np.mean(mape)\n",
    "    smape = np.mean(smape)\n",
    "    max = np.mean(max)\n",
    "    mae = np.mean(mae)\n",
    "    mse = np.mean(mse)\n",
    "\n",
    "    # Collect the results\n",
    "    results.append({\n",
    "        'pollutant': pollutant,\n",
    "        'freq': 'D',\n",
    "        'fold': max_k_fold,\n",
    "        'train_len': f'{(len(obj.nixtla_df))-(max_k_fold*horizon)} - {(len(obj.nixtla_df))-(0*horizon)}',\n",
    "        'h': horizon,\n",
    "        'input_size': input_size,\n",
    "        'n_stacks': n_stacks,\n",
    "        'n_blocks': n_blocks,\n",
    "        'max_steps': max_steps,\n",
    "        'local_scalar_type': local_scalar_type,\n",
    "        'interpretability': interpretability,\n",
    "        'mape': mape,\n",
    "        'smape': smape,\n",
    "        'max': max,\n",
    "        'mae': mae,\n",
    "        'mse': mse,\n",
    "    })\n",
    "\n",
    "    # The objective for Optuna is to minimize the MAE (or maximize a metric)\n",
    "    return smape, mae  # Any metric you want to optimize\n",
    "\n",
    "for pollutant in data[['pm10']]:\n",
    "    for h in [14, 30, 60, 90, 120]:\n",
    "        # Initialize the results list\n",
    "        results = []\n",
    "        # Define the optimization study_nbeats\n",
    "        study_nbeats = optuna.create_study(directions=['minimize','minimize'])  # Minimize the MAE\n",
    "\n",
    "        # Run the optimization with the number of trials you want\n",
    "        study_nbeats.optimize(partial(objective, pollutant=pollutant, horizon=h), n_trials=3)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        NBEATS_W = pd.DataFrame(results)\n",
    "\n",
    "        output_dir = fr'Results COBEQ\\NBEATS (D)\\{pollutant}'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        NBEATS_W.to_pickle(fr'Results COBEQ\\NBEATS (D)\\{pollutant}\\{h}D_Df.pkl')\n",
    "        joblib.dump(study_nbeats, fr\"Results COBEQ\\NBEATS (D)\\{pollutant}\\{h}D_Study.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Statistical**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA, AutoCES, AutoETS, AutoTheta\n",
    "\n",
    "for pollutant in ['pm10',]:\n",
    "    for h in [14, 90]: #  30, 60, 90, 120\n",
    "        results_stats = pd.DataFrame()\n",
    "\n",
    "        mape = []\n",
    "        smape = []\n",
    "        max = []\n",
    "        mae = []\n",
    "        mse = []\n",
    "\n",
    "        max_k_fold = 5\n",
    "        for k_fold in range(0,max_k_fold):\n",
    "            print(f'\\nPollutant = {pollutant} \\nh = {h}\\nFold = {k_fold+1}\\n')\n",
    "            # Instantiate TimeObject and prepare training data\n",
    "            obj = TimeObject(df=data, column=pollutant, agg_freq='D')\n",
    "            obj.fixed_origin_rolling_window_cross_validation(\n",
    "                initial_train_date=None,\n",
    "                total_data_points=(len(obj.nixtla_df))-(k_fold*h),\n",
    "                h=h,\n",
    "                plot_interval=False\n",
    "            )\n",
    "\n",
    "            season_length = 365 # Monthly data \n",
    "\n",
    "            models = [\n",
    "                AutoARIMA(season_length=season_length, alias='AutoARIMA'),\n",
    "                AutoCES(season_length=season_length, model='Z', alias='AutoCES-Z'),\n",
    "                AutoCES(season_length=season_length, model='S', alias='AutoCES-S'),\n",
    "                AutoCES(season_length=season_length, model='P', alias='AutoCES-P'),\n",
    "                AutoCES(season_length=season_length, model='N', alias='AutoCES-N'),\n",
    "                AutoTheta(season_length=season_length, decomposition_type=\"multiplicative\", alias='AutoTheta-Multi'),\n",
    "                AutoTheta(season_length=season_length, decomposition_type=\"additive\", alias='AutoTheta-Add'),\n",
    "            ]\n",
    "            models = models + [\n",
    "                AutoETS(season_length=season_length, model=ets, alias=f'AutoETS-{ets}')\n",
    "                for ets in [f\"{e}{t}{s}\" for e in ['Z',]\n",
    "                            for t in ['Z', 'A', 'N'] \n",
    "                            for s in ['Z', 'A', 'M', 'N'] \n",
    "            ]]\n",
    "\n",
    "            frct = StatsForecast(models=models, freq='D')\n",
    "            frct.fit(df=obj.Y_train)\n",
    "            predicted = frct.predict(h=h)\n",
    "\n",
    "            columns = predicted.columns\n",
    "            columns = columns[(columns != 'ds') & (columns != 'unique_id')]\n",
    "\n",
    "            for method in columns:\n",
    "                obj.metrics_(predicted, method=method)\n",
    "                results_stats = pd.concat([results_stats, pd.DataFrame({\n",
    "                    'pollutant': [pollutant],\n",
    "                    'method': [method],\n",
    "                    'freq': ['D'],\n",
    "                    'h': [h],\n",
    "                    'mape': [obj.metrics['mape']],\n",
    "                    'smape': [obj.metrics['smape']],\n",
    "                    'max': [obj.metrics['max']],\n",
    "                    'mae': [obj.metrics['mae']],\n",
    "                    'mse': [obj.metrics['mse']]\n",
    "                })])\n",
    "            \n",
    "            # ======================================================================================================\n",
    "\n",
    "            nbeats = joblib.load(fr\"Results COBEQ\\NBEATS (D)\\{pollutant}\\{h}D_Study.pkl\")\n",
    "            model = NBEATS(\n",
    "                h=h,\n",
    "                input_size=nbeats.best_trials[0].params.get('input_size'),\n",
    "                stack_types=nbeats.best_trials[0].params.get('interpretability')+(nbeats.best_trials[0].params.get('n_stacks')-len(nbeats.best_trials[0].params.get('interpretability')))*['identity'],\n",
    "                n_blocks=nbeats.best_trials[0].params.get('n_stacks') * [nbeats.best_trials[0].params.get('n_blocks')],\n",
    "                max_steps=nbeats.best_trials[0].params.get('max_steps'),\n",
    "                learning_rate=1e-3,\n",
    "                val_check_steps=10,\n",
    "            )\n",
    "            fcst = NeuralForecast(\n",
    "                models=[model],\n",
    "                freq='D',\n",
    "                local_scaler_type=nbeats.best_trials[0].params.get('local_scalar_type')\n",
    "            )\n",
    "            fcst.fit(df=obj.Y_train, verbose=False)\n",
    "            predicted = fcst.predict(df=obj.Y_train, verbose=False)\n",
    "            obj.metrics_(predicted, method='NBEATS')\n",
    "            results_stats = pd.concat([results_stats, pd.DataFrame({\n",
    "                'pollutant': [pollutant],\n",
    "                'method': ['NBEATS'],\n",
    "                'freq': ['D'],\n",
    "                'h': [h],\n",
    "                'mape': [obj.metrics['mape']],\n",
    "                'smape': [obj.metrics['smape']],\n",
    "                'max': [obj.metrics['max']],\n",
    "                'mae': [obj.metrics['mae']],\n",
    "                'mse': [obj.metrics['mse']]\n",
    "            })])\n",
    "\n",
    "            # ======================================================================================================\n",
    "            \n",
    "            nhits = joblib.load(fr\"Results COBEQ\\NHITS (D)\\{pollutant}\\{h}D_Study.pkl\")\n",
    "            model = NHITS(\n",
    "                h=h,\n",
    "                input_size=nhits.best_trials[0].params.get('input_size'),\n",
    "                stack_types=nhits.best_trials[0].params.get('n_stacks')*['identity'],\n",
    "                n_freq_downsample=nhits.best_trials[0].params.get('n_freq_downsample')+(nhits.best_trials[0].params.get('n_stacks')-len(nhits.best_trials[0].params.get('n_freq_downsample')))*[1],\n",
    "                n_blocks=nhits.best_trials[0].params.get('n_stacks')*[nhits.best_trials[0].params.get('n_blocks')],\n",
    "                n_pool_kernel_size=(nhits.best_trials[0].params.get('n_stacks')-len(nhits.best_trials[0].params.get('n_pool_kernel_size')))*[1]+nhits.best_trials[0].params.get('n_pool_kernel_size'),\n",
    "                pooling_mode=\"MaxPool1d\",\n",
    "                activation=\"ReLU\",\n",
    "                interpolation_mode='linear',\n",
    "                max_steps=nhits.best_trials[0].params.get('max_steps'),\n",
    "                val_check_steps=10,\n",
    "            )\n",
    "            fcst = NeuralForecast(\n",
    "                models=[model],\n",
    "                freq='D',\n",
    "                local_scaler_type=nhits.best_trials[0].params.get('local_scalar_type')\n",
    "            )\n",
    "            fcst.fit(df=obj.Y_train, verbose=False)\n",
    "            predicted = fcst.predict(df=obj.Y_train, verbose=False)\n",
    "            obj.metrics_(predicted, method='NHITS')\n",
    "            results_stats = pd.concat([results_stats, pd.DataFrame({\n",
    "                'pollutant': [pollutant],\n",
    "                'method': ['NHITS'],\n",
    "                'freq': ['D'],\n",
    "                'h': [h],\n",
    "                'mape': [obj.metrics['mape']],\n",
    "                'smape': [obj.metrics['smape']],\n",
    "                'max': [obj.metrics['max']],\n",
    "                'mae': [obj.metrics['mae']],\n",
    "                'mse': [obj.metrics['mse']]\n",
    "            })])\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "\n",
    "        # ======================================================================================================\n",
    "\n",
    "        results_stats = pd.DataFrame(results_stats)\n",
    "\n",
    "        output_dir = fr'Results COBEQ\\Stats (D)\\{pollutant}'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        results_stats.to_pickle(fr'Results COBEQ\\Stats (D)\\{pollutant}\\{h}D_Df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results_stats.groupby(['pollutant','method','h','freq'])[['mape', 'smape', 'max', 'mae', 'mse']].mean().reset_index().sort_values('smape')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for model in ['NHITS','NBEATS']:\n",
    "    for H in [12,26,52,78]:\n",
    "        x = joblib.load(fr'Results COBEQ/Stats (W)/pm10/{H}W_Df.pkl').sort_values(['smape','mae']).reset_index()\n",
    "        metrics = ['smape', 'mae', 'mape', 'mse', 'max']\n",
    "        for metric in metrics:\n",
    "            x[f'{metric}_rank'] = x[metric].rank(method='min')\n",
    "\n",
    "        # If you want best = 1, worst = 21, and ranks to be integers\n",
    "        x[[f'{m}_rank' for m in metrics]] = x[[f'{m}_rank' for m in metrics]].astype(int)\n",
    "        df = pd.concat([df, x[x['method'] == model]])\n",
    "        del df['index']\n",
    "df.to_excel(r'x.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for model in ['NHITS','NBEATS']:\n",
    "    for H in [12,26,52,78]:\n",
    "        x = joblib.load(fr'C:\\Users\\gustavo.filho\\Documents\\Python\\GitHub\\TCC\\Results COBEQ\\{model} (W)\\pm10\\{H}W_Df.pkl').sort_values(['smape','mae']).reset_index()\n",
    "        df = pd.concat([df, pd.DataFrame(x.iloc[0,:]).T])\n",
    "\n",
    "df.to_excel(r'x.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA, AutoCES, AutoETS, AutoTheta\n",
    "\n",
    "results_stats = []\n",
    "for pollutant in ['pm10']:\n",
    "    for h in [365]:\n",
    "\n",
    "        obj = TimeObject(data, pollutant, agg_freq='D')\n",
    "        obj.NIXTLA_train_test(split=h)\n",
    "\n",
    "        season_length = 365 # Monthly data \n",
    "        horizon = len(obj.Y_train) # number of predictions\n",
    "\n",
    "        models = [\n",
    "            # AutoARIMA(season_length=season_length, alias='AutoARIMA'),\n",
    "            AutoCES(season_length=season_length, model='Z', alias='AutoCES-Z'),\n",
    "            AutoCES(season_length=season_length, model='S', alias='AutoCES-S'),\n",
    "            AutoCES(season_length=season_length, model='P', alias='AutoCES-P'),\n",
    "            AutoCES(season_length=season_length, model='N', alias='AutoCES-N'),\n",
    "            AutoTheta(season_length=season_length, decomposition_type=\"multiplicative\", alias='AutoTheta-Multi'),\n",
    "            AutoTheta(season_length=season_length, decomposition_type=\"additive\", alias='AutoTheta-Add'),\n",
    "        ]\n",
    "        models = models + [\n",
    "            AutoETS(season_length=season_length, model=ets, alias=f'AutoETS-{ets}')\n",
    "            for ets in [f\"{e}{t}{s}\" for e in ['Z',]\n",
    "                        for t in ['Z', 'A', 'N'] \n",
    "                        for s in ['Z', 'A', 'M', 'N'] \n",
    "        ]]\n",
    "\n",
    "        frct = StatsForecast(models=models, freq='D')\n",
    "        frct.fit(df=obj.Y_train)\n",
    "        predicted = frct.predict(h=h)\n",
    "\n",
    "        columns = predicted.columns\n",
    "        columns = columns[(columns != 'ds') & (columns != 'unique_id')]\n",
    "\n",
    "        results_stats = pd.DataFrame()\n",
    "        for method in columns:\n",
    "            obj.plot_forecast(predicted, show_metrics=True, method=method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pollutant in ['pm10']:\n",
    "    for h in [360]:\n",
    "\n",
    "        obj = TimeObject(data, pollutant, agg_freq='D')\n",
    "        obj.NIXTLA_train_test(split=h)\n",
    "        horizon = len(obj.Y_train) # number of predictions\n",
    "\n",
    "        # ======================================================================================================\n",
    "\n",
    "        nbeats = joblib.load(fr\"Results COBEQ\\NBEATS (W)\\{pollutant}\\78W_Study.pkl\")\n",
    "        model = NBEATS(\n",
    "            h=h,\n",
    "            input_size=nbeats.best_trials[0].params.get('input_size'),\n",
    "            stack_types=nbeats.best_trials[0].params.get('interpretability')+(nbeats.best_trials[0].params.get('n_stacks')-len(nbeats.best_trials[0].params.get('interpretability')))*['identity'],\n",
    "            n_blocks=nbeats.best_trials[0].params.get('n_stacks') * [nbeats.best_trials[0].params.get('n_blocks')],\n",
    "            max_steps=nbeats.best_trials[0].params.get('max_steps'),\n",
    "            learning_rate=1e-3,\n",
    "            val_check_steps=10,\n",
    "        )\n",
    "        fcst = NeuralForecast(\n",
    "            models=[model],\n",
    "            freq='D',\n",
    "            local_scaler_type=nbeats.best_trials[0].params.get('local_scalar_type')\n",
    "        )\n",
    "        fcst.fit(df=obj.Y_train, verbose=False)\n",
    "        predicted = fcst.predict(df=obj.Y_train, verbose=False)\n",
    "        obj.plot_forecast(predicted, method='NBEATS', show_metrics=True)\n",
    "\n",
    "        # ======================================================================================================\n",
    "        \n",
    "        nhits = joblib.load(fr\"Results COBEQ\\NHITS (W)\\{pollutant}\\78W_Study.pkl\")\n",
    "        model = NHITS(\n",
    "            h=h,\n",
    "            input_size=nhits.best_trials[0].params.get('input_size'),\n",
    "            stack_types=nhits.best_trials[0].params.get('n_stacks')*['identity'],\n",
    "            n_freq_downsample=nhits.best_trials[0].params.get('n_freq_downsample')+(nhits.best_trials[0].params.get('n_stacks')-len(nhits.best_trials[0].params.get('n_freq_downsample')))*[1],\n",
    "            n_blocks=nhits.best_trials[0].params.get('n_stacks')*[nhits.best_trials[0].params.get('n_blocks')],\n",
    "            n_pool_kernel_size=(nhits.best_trials[0].params.get('n_stacks')-len(nhits.best_trials[0].params.get('n_pool_kernel_size')))*[1]+nhits.best_trials[0].params.get('n_pool_kernel_size'),\n",
    "            pooling_mode=\"MaxPool1d\",\n",
    "            activation=\"ReLU\",\n",
    "            interpolation_mode='linear',\n",
    "            max_steps=nhits.best_trials[0].params.get('max_steps'),\n",
    "            val_check_steps=10,\n",
    "        )\n",
    "        fcst = NeuralForecast(\n",
    "            models=[model],\n",
    "            freq='D',\n",
    "            local_scaler_type=nhits.best_trials[0].params.get('local_scalar_type')\n",
    "        )\n",
    "        fcst.fit(df=obj.Y_train, verbose=False)\n",
    "        predicted = fcst.predict(df=obj.Y_train, verbose=False)\n",
    "        obj.plot_forecast(predicted, show_metrics=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
