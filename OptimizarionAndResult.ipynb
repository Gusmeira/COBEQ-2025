{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **COBEQ 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "import optuna\n",
    "import itertools\n",
    "import shutil\n",
    "from functools import partial\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.subplots\n",
    "import time\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NBEATS, NHITS\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "ponte = pd.read_pickle(r'Data\\Data_Ponte_dos_Remedios.pkl')\n",
    "del ponte['o3']\n",
    "guarulhos = pd.read_pickle(r'Data\\Data_Guarulhos.pkl')\n",
    "guarulhos = guarulhos[['date','o3']]\n",
    "\n",
    "data = ponte.merge(guarulhos, on='date', how='outer')\n",
    "data.reset_index(drop=True)\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "os.environ['NIXTLA_ID_AS_COL'] = '1'\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "trainer = Trainer(\n",
    "    max_steps=4,\n",
    "    logger=False,\n",
    "    enable_progress_bar=False,\n",
    "    enable_model_summary=False  # Disable model summary\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"optuna\")\n",
    "\n",
    "from TimeObjectModule import TimeObject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Experimental Planning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Optuna**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-HiTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-25 10:13:56,508] Trial 299 finished with values: [21.40198, 5.75624] and parameters: {'input_size': 67, 'n_stacks': 3, 'n_blocks': 7, 'max_steps': 624, 'local_scalar_type': 'standard', 'n_pool_kernel_size': [1, 1, 1], 'n_freq_downsample': [12, 1, 12]}.\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function\n",
    "def objective(trial, pollutant, horizon):\n",
    "    # Hyperparameter search space\n",
    "    input_size = trial.suggest_int('input_size', 4, 156, step=1)\n",
    "    n_stacks = trial.suggest_int('n_stacks', 3, 7, step=1)\n",
    "    n_blocks = trial.suggest_int('n_blocks', 1, 7, step=1)\n",
    "    max_steps = trial.suggest_int('max_steps', 10, 700, step=1)\n",
    "    local_scalar_type = trial.suggest_categorical('local_scalar_type', [None, 'standard', 'boxcox', 'minmax'])\n",
    "    n_pool_kernel_size = trial.suggest_categorical('n_pool_kernel_size', [list(combination) for combination in list(itertools.product([1, 2, 3], repeat=3))])\n",
    "    n_freq_downsample = trial.suggest_categorical('n_freq_downsample', [list(combination) for combination in list(itertools.product([1, 4, 12, 52], repeat=3))])\n",
    "\n",
    "    mape = []\n",
    "    smape = []\n",
    "    max = []\n",
    "    mae = []\n",
    "    mse = []\n",
    "    # Split for cross validation\n",
    "    for split in [1,184,366]:\n",
    "        print(f'\\nPollutant = {pollutant} \\nh = {horizon} \\nTrial = {trial.number+1}\\n')\n",
    "        # Instantiate TimeObject and prepare training data\n",
    "        obj = TimeObject(df=data[:-split], column=pollutant, agg_freq='W')\n",
    "        obj.NIXTLA_train_test(split=horizon)\n",
    "\n",
    "        # Define the model\n",
    "        model = NHITS(\n",
    "            h=horizon,\n",
    "            input_size=input_size,\n",
    "            stack_types=n_stacks*['identity'],\n",
    "            n_freq_downsample=n_freq_downsample+(n_stacks-len(n_freq_downsample))*[1],\n",
    "            n_blocks=n_stacks*[n_blocks],\n",
    "            n_pool_kernel_size=(n_stacks-len(n_pool_kernel_size))*[1]+n_pool_kernel_size,\n",
    "            pooling_mode=\"MaxPool1d\",\n",
    "            activation=\"ReLU\",\n",
    "            interpolation_mode='linear',\n",
    "            max_steps=max_steps,\n",
    "            val_check_steps=10,\n",
    "            early_stop_patience_steps=int(np.round(max_steps/(20),0)),\n",
    "        )\n",
    "\n",
    "        # Initialize NeuralForecast and fit the model\n",
    "        fcst = NeuralForecast(\n",
    "            models=[model],\n",
    "            freq='W',\n",
    "            local_scaler_type=local_scalar_type\n",
    "        )\n",
    "        fcst.fit(df=obj.Y_train, verbose=False, val_size=horizon+1)\n",
    "        prediction = fcst.predict(df=obj.Y_train, verbose=False)\n",
    "\n",
    "        # Evaluate metrics\n",
    "        obj.metrics_(forecast_df=prediction, method='NHITS')\n",
    "        mape.append(obj.metrics['mape'])\n",
    "        smape.append(obj.metrics['smape'])\n",
    "        max.append(obj.metrics['max'])\n",
    "        mae.append(obj.metrics['mae'])\n",
    "        mse.append(obj.metrics['mse'])\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "\n",
    "    try:\n",
    "        directory_path = \"lightning_logs\"\n",
    "        if os.path.exists(directory_path):\n",
    "            shutil.rmtree(directory_path)\n",
    "    except:\n",
    "        ...\n",
    "\n",
    "    mape = np.mean(mape)\n",
    "    smape = np.mean(smape)\n",
    "    max = np.mean(max)\n",
    "    mae = np.mean(mae)\n",
    "    mse = np.mean(mse)\n",
    "\n",
    "    # Collect the results\n",
    "    results.append({\n",
    "        'poll': pollutant,\n",
    "        'freq': 'W',\n",
    "        'split': split,\n",
    "        'h': horizon,\n",
    "        'input_size': input_size,\n",
    "        'n_stacks': n_stacks,\n",
    "        'n_blocks': n_blocks,\n",
    "        'max_steps': max_steps,\n",
    "        'local_scalar_type': local_scalar_type,\n",
    "        'n_pool_kernel_size': n_pool_kernel_size,\n",
    "        'n_freq_downsample': n_freq_downsample,\n",
    "        'mape': mape,\n",
    "        'smape': smape,\n",
    "        'max': max,\n",
    "        'mae': mae,\n",
    "        'mse': mse,\n",
    "    })\n",
    "\n",
    "    # The objective for Optuna is to minimize the MAE (or maximize a metric)\n",
    "    return smape, mae  # Any metric you want to optimize\n",
    "\n",
    "for pollutant in data[['pm10']]:\n",
    "    for h in [12, 26, 52, 78]:\n",
    "        # Initialize the results list\n",
    "        results = []\n",
    "        # Define the optimization study_nhits\n",
    "        study_nhits = optuna.create_study(directions=['minimize','minimize'])  # Minimize the MAE\n",
    "\n",
    "        # Run the optimization with the number of trials you want\n",
    "        study_nhits.optimize(partial(objective, pollutant=pollutant, horizon=h), n_trials=300)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        NHITS_W = pd.DataFrame(results)\n",
    "\n",
    "        output_dir = fr'Results COBEQ\\NHITS (W)\\{pollutant}'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        NHITS_W.to_pickle(fr'Results COBEQ\\NHITS (W)\\{pollutant}\\{h}W_Df.pkl')\n",
    "        joblib.dump(study_nhits, fr\"Results COBEQ\\NHITS (W)\\{pollutant}\\{h}W_Study.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NBEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-25 13:53:03,863] Trial 299 finished with values: [21.49116, 5.70076] and parameters: {'input_size': 86, 'n_stacks': 5, 'n_blocks': 3, 'max_steps': 616, 'local_scalar_type': 'boxcox', 'interpretability': ['trend', 'trend']}.\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function\n",
    "def objective(trial, pollutant, horizon):\n",
    "    # Hyperparameter search space\n",
    "    input_size = trial.suggest_int('input_size', 4, 156, step=1)\n",
    "    n_stacks = trial.suggest_int('n_stacks', 2, 7, step=1)\n",
    "    n_blocks = trial.suggest_int('n_blocks', 1, 5, step=1)\n",
    "    max_steps = trial.suggest_int('max_steps', 10, 700, step=1)\n",
    "    local_scalar_type = trial.suggest_categorical('local_scalar_type', [None, 'standard', 'boxcox', 'minmax'])\n",
    "    interpretability = trial.suggest_categorical('interpretability', [list(combination) for combination in list(itertools.product(['seasonality', 'trend', 'identity'], repeat=2))])\n",
    "\n",
    "    mape = []\n",
    "    smape = []\n",
    "    max = []\n",
    "    mae = []\n",
    "    mse = []\n",
    "    # Split for cross validation\n",
    "    for split in [1]:\n",
    "        print(f'\\nPollutant = {pollutant} \\nh = {horizon} \\nTrial = {trial.number+1}\\n')\n",
    "        # Instantiate TimeObject and prepare training data\n",
    "        obj = TimeObject(df=data[:-split], column=pollutant, agg_freq='W')\n",
    "        obj.NIXTLA_train_test(split=horizon)\n",
    "\n",
    "        # Define the model\n",
    "        model = NBEATS(\n",
    "            h=horizon,\n",
    "            input_size=input_size,\n",
    "            stack_types=interpretability+(n_stacks-len(interpretability))*['identity'],\n",
    "            n_blocks=n_stacks * [n_blocks],\n",
    "            max_steps=max_steps,\n",
    "            learning_rate=1e-3,\n",
    "            val_check_steps=10,\n",
    "            early_stop_patience_steps=int(np.round(max_steps/(20),0)),\n",
    "        )\n",
    "\n",
    "        # Initialize NeuralForecast and fit the model\n",
    "        fcst = NeuralForecast(\n",
    "            models=[model],\n",
    "            freq='W',\n",
    "            local_scaler_type=local_scalar_type\n",
    "        )\n",
    "        fcst.fit(df=obj.Y_train, verbose=False, val_size=horizon+1)\n",
    "        prediction = fcst.predict(df=obj.Y_train, verbose=False)\n",
    "\n",
    "        # Evaluate metrics\n",
    "        obj.metrics_(forecast_df=prediction, method='NBEATS')\n",
    "        mape.append(obj.metrics['mape'])\n",
    "        smape.append(obj.metrics['smape'])\n",
    "        max.append(obj.metrics['max'])\n",
    "        mae.append(obj.metrics['mae'])\n",
    "        mse.append(obj.metrics['mse'])\n",
    "            \n",
    "        clear_output(wait=True)\n",
    "\n",
    "        try:\n",
    "            directory_path = \"lightning_logs\"\n",
    "            if os.path.exists(directory_path):\n",
    "                shutil.rmtree(directory_path)\n",
    "        except:\n",
    "            ...\n",
    "\n",
    "    mape = np.mean(mape)\n",
    "    smape = np.mean(smape)\n",
    "    max = np.mean(max)\n",
    "    mae = np.mean(mae)\n",
    "    mse = np.mean(mse)\n",
    "\n",
    "    # Collect the results\n",
    "    results.append({\n",
    "        'pollutant': pollutant,\n",
    "        'freq': 'W',\n",
    "        'split': split,\n",
    "        'h': horizon,\n",
    "        'input_size': input_size,\n",
    "        'n_stacks': n_stacks,\n",
    "        'n_blocks': n_blocks,\n",
    "        'max_steps': max_steps,\n",
    "        'local_scalar_type': local_scalar_type,\n",
    "        'interpretability': interpretability,\n",
    "        'mape': mape,\n",
    "        'smape': smape,\n",
    "        'max': max,\n",
    "        'mae': mae,\n",
    "        'mse': mse,\n",
    "    })\n",
    "\n",
    "    # The objective for Optuna is to minimize the MAE (or maximize a metric)\n",
    "    return smape, mae  # Any metric you want to optimize\n",
    "\n",
    "for pollutant in data[['pm10']]:\n",
    "    for h in [12, 26, 52, 78]:\n",
    "        # Initialize the results list\n",
    "        results = []\n",
    "        # Define the optimization study_nbeats\n",
    "        study_nbeats = optuna.create_study(directions=['minimize','minimize'])  # Minimize the MAE\n",
    "\n",
    "        # Run the optimization with the number of trials you want\n",
    "        study_nbeats.optimize(partial(objective, pollutant=pollutant, horizon=h), n_trials=300)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        NBEATS_W = pd.DataFrame(results)\n",
    "\n",
    "        output_dir = fr'Results COBEQ\\NBEATS (W)\\{pollutant}'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        NBEATS_W.to_pickle(fr'Results COBEQ\\NBEATS (W)\\{pollutant}\\{h}W_Df.pkl')\n",
    "        joblib.dump(study_nbeats, fr\"Results COBEQ\\NBEATS (W)\\{pollutant}\\{h}W_Study.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Statistical**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA, AutoCES, AutoETS, AutoTheta\n",
    "\n",
    "results_stats = []\n",
    "for pollutant in ['pm10']:\n",
    "    for h in [12, 26, 52, 78]:\n",
    "\n",
    "        obj = TimeObject(data, pollutant, agg_freq='W')\n",
    "        obj.NIXTLA_train_test(split=h)\n",
    "\n",
    "        season_length = 52 # Monthly data \n",
    "        horizon = len(obj.Y_train) # number of predictions\n",
    "\n",
    "        models = [\n",
    "            # AutoARIMA(season_length=season_length, alias='AutoARIMA'),\n",
    "            AutoCES(season_length=season_length, model='Z', alias='AutoCES-Z'),\n",
    "            AutoCES(season_length=season_length, model='S', alias='AutoCES-S'),\n",
    "            AutoCES(season_length=season_length, model='P', alias='AutoCES-P'),\n",
    "            AutoCES(season_length=season_length, model='N', alias='AutoCES-N'),\n",
    "            AutoTheta(season_length=season_length, decomposition_type=\"multiplicative\", alias='AutoTheta-Multi'),\n",
    "            AutoTheta(season_length=season_length, decomposition_type=\"additive\", alias='AutoTheta-Add'),\n",
    "        ]\n",
    "        models = models + [\n",
    "            AutoETS(season_length=season_length, model=ets, alias=f'AutoETS-{ets}')\n",
    "            for ets in [f\"{e}{t}{s}\" for e in ['Z',]\n",
    "                        for t in ['Z', 'A', 'N'] \n",
    "                        for s in ['Z', 'A', 'M', 'N'] \n",
    "        ]]\n",
    "\n",
    "        frct = StatsForecast(models=models, freq='W')\n",
    "        frct.fit(df=obj.Y_train)\n",
    "        predicted = frct.predict(h=h)\n",
    "\n",
    "        columns = predicted.columns\n",
    "        columns = columns[(columns != 'ds') & (columns != 'unique_id')]\n",
    "\n",
    "        results_stats = pd.DataFrame()\n",
    "        for method in columns:\n",
    "            obj.metrics_(predicted, method=method)\n",
    "            results_stats = pd.concat([results_stats, pd.DataFrame({\n",
    "                'pollutant': [pollutant],\n",
    "                'method': [method],\n",
    "                'freq': ['W'],\n",
    "                'h': [h],\n",
    "                'mape': [obj.metrics['mape']],\n",
    "                'smape': [obj.metrics['smape']],\n",
    "                'max': [obj.metrics['max']],\n",
    "                'mae': [obj.metrics['mae']],\n",
    "                'mse': [obj.metrics['mse']]\n",
    "            })])\n",
    "        \n",
    "        # ======================================================================================================\n",
    "\n",
    "        nbeats = joblib.load(fr\"Results COBEQ\\NBEATS (W)\\{pollutant}\\{h}W_Study.pkl\")\n",
    "        model = NBEATS(\n",
    "            h=h,\n",
    "            input_size=nbeats.best_trials[0].params.get('input_size'),\n",
    "            stack_types=nbeats.best_trials[0].params.get('interpretability')+(nbeats.best_trials[0].params.get('n_stacks')-len(nbeats.best_trials[0].params.get('interpretability')))*['identity'],\n",
    "            n_blocks=nbeats.best_trials[0].params.get('n_stacks') * [nbeats.best_trials[0].params.get('n_blocks')],\n",
    "            max_steps=nbeats.best_trials[0].params.get('max_steps'),\n",
    "            learning_rate=1e-3,\n",
    "            val_check_steps=10,\n",
    "        )\n",
    "        fcst = NeuralForecast(\n",
    "            models=[model],\n",
    "            freq='W',\n",
    "            local_scaler_type=nbeats.best_trials[0].params.get('local_scalar_type')\n",
    "        )\n",
    "        fcst.fit(df=obj.Y_train, verbose=False)\n",
    "        predicted = fcst.predict(df=obj.Y_train, verbose=False)\n",
    "        obj.metrics_(predicted, method='NBEATS')\n",
    "        results_stats = pd.concat([results_stats, pd.DataFrame({\n",
    "            'pollutant': [pollutant],\n",
    "            'method': ['NBEATS'],\n",
    "            'freq': ['W'],\n",
    "            'h': [h],\n",
    "            'mape': [obj.metrics['mape']],\n",
    "            'smape': [obj.metrics['smape']],\n",
    "            'max': [obj.metrics['max']],\n",
    "            'mae': [obj.metrics['mae']],\n",
    "            'mse': [obj.metrics['mse']]\n",
    "        })])\n",
    "\n",
    "        # ======================================================================================================\n",
    "        \n",
    "        nhits = joblib.load(fr\"Results COBEQ\\NHITS (W)\\{pollutant}\\{h}W_Study.pkl\")\n",
    "        model = NHITS(\n",
    "            h=h,\n",
    "            input_size=nhits.best_trials[0].params.get('input_size'),\n",
    "            stack_types=nhits.best_trials[0].params.get('n_stacks')*['identity'],\n",
    "            n_freq_downsample=nhits.best_trials[0].params.get('n_freq_downsample')+(nhits.best_trials[0].params.get('n_stacks')-len(nhits.best_trials[0].params.get('n_freq_downsample')))*[1],\n",
    "            n_blocks=nhits.best_trials[0].params.get('n_stacks')*[nhits.best_trials[0].params.get('n_blocks')],\n",
    "            n_pool_kernel_size=(nhits.best_trials[0].params.get('n_stacks')-len(nhits.best_trials[0].params.get('n_pool_kernel_size')))*[1]+nhits.best_trials[0].params.get('n_pool_kernel_size'),\n",
    "            pooling_mode=\"MaxPool1d\",\n",
    "            activation=\"ReLU\",\n",
    "            interpolation_mode='linear',\n",
    "            max_steps=nhits.best_trials[0].params.get('max_steps'),\n",
    "            val_check_steps=10,\n",
    "        )\n",
    "        fcst = NeuralForecast(\n",
    "            models=[model],\n",
    "            freq='W',\n",
    "            local_scaler_type=nhits.best_trials[0].params.get('local_scalar_type')\n",
    "        )\n",
    "        fcst.fit(df=obj.Y_train, verbose=False)\n",
    "        predicted = fcst.predict(df=obj.Y_train, verbose=False)\n",
    "        obj.metrics_(predicted, method='NHITS')\n",
    "        results_stats = pd.concat([results_stats, pd.DataFrame({\n",
    "            'pollutant': [pollutant],\n",
    "            'method': ['NHITS'],\n",
    "            'freq': ['W'],\n",
    "            'h': [h],\n",
    "            'mape': [obj.metrics['mape']],\n",
    "            'smape': [obj.metrics['smape']],\n",
    "            'max': [obj.metrics['max']],\n",
    "            'mae': [obj.metrics['mae']],\n",
    "            'mse': [obj.metrics['mse']]\n",
    "        })])\n",
    "\n",
    "        # ======================================================================================================\n",
    "\n",
    "        results_stats = pd.DataFrame(results_stats)\n",
    "        # display(results_stats)\n",
    "\n",
    "        output_dir = fr'Results COBEQ\\Stats (W)\\{pollutant}'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        results_stats.to_pickle(fr'Results COBEQ\\Stats (W)\\{pollutant}\\{h}W_Df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "pollutant",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "method",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "freq",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "h",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "mape",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "smape",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "max",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mae",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mse",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "smape_rank",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "mae_rank",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "mape_rank",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "mse_rank",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "max_rank",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "bd3a35f6-cc9b-4364-8a01-d1285af9a202",
       "rows": [
        [
         "11",
         "pm10",
         "NHITS",
         "W",
         "12",
         "21.61003",
         "15.47986",
         "12.52267",
         "3.15901",
         "20.76394",
         "12",
         "12",
         "12",
         "12",
         "12"
        ],
        [
         "10",
         "pm10",
         "NHITS",
         "W",
         "26",
         "23.89276",
         "21.12368",
         "25.26393",
         "5.52537",
         "58.60768",
         "11",
         "4",
         "3",
         "11",
         "14"
        ],
        [
         "6",
         "pm10",
         "NHITS",
         "W",
         "52",
         "20.25154",
         "18.99953",
         "15.63252",
         "4.86126",
         "39.63625",
         "7",
         "2",
         "4",
         "1",
         "7"
        ],
        [
         "11",
         "pm10",
         "NHITS",
         "W",
         "78",
         "29.27972",
         "25.55316",
         "30.36795",
         "6.92467",
         "91.43602",
         "12",
         "12",
         "12",
         "12",
         "1"
        ],
        [
         "12",
         "pm10",
         "NBEATS",
         "W",
         "12",
         "23.12074",
         "16.27287",
         "12.99588",
         "3.31098",
         "23.21854",
         "13",
         "13",
         "13",
         "13",
         "13"
        ],
        [
         "9",
         "pm10",
         "NBEATS",
         "W",
         "26",
         "24.88597",
         "21.07951",
         "15.33145",
         "5.5996",
         "54.09961",
         "10",
         "7",
         "7",
         "6",
         "8"
        ],
        [
         "9",
         "pm10",
         "NBEATS",
         "W",
         "52",
         "21.21281",
         "19.80284",
         "16.39186",
         "5.01307",
         "43.11202",
         "10",
         "7",
         "8",
         "6",
         "9"
        ],
        [
         "12",
         "pm10",
         "NBEATS",
         "W",
         "78",
         "31.24361",
         "28.22053",
         "36.15615",
         "7.37499",
         "104.68935",
         "13",
         "13",
         "14",
         "13",
         "18"
        ]
       ],
       "shape": {
        "columns": 14,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pollutant</th>\n",
       "      <th>method</th>\n",
       "      <th>freq</th>\n",
       "      <th>h</th>\n",
       "      <th>mape</th>\n",
       "      <th>smape</th>\n",
       "      <th>max</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>smape_rank</th>\n",
       "      <th>mae_rank</th>\n",
       "      <th>mape_rank</th>\n",
       "      <th>mse_rank</th>\n",
       "      <th>max_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pm10</td>\n",
       "      <td>NHITS</td>\n",
       "      <td>W</td>\n",
       "      <td>12</td>\n",
       "      <td>21.61003</td>\n",
       "      <td>15.47986</td>\n",
       "      <td>12.52267</td>\n",
       "      <td>3.15901</td>\n",
       "      <td>20.76394</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pm10</td>\n",
       "      <td>NHITS</td>\n",
       "      <td>W</td>\n",
       "      <td>26</td>\n",
       "      <td>23.89276</td>\n",
       "      <td>21.12368</td>\n",
       "      <td>25.26393</td>\n",
       "      <td>5.52537</td>\n",
       "      <td>58.60768</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pm10</td>\n",
       "      <td>NHITS</td>\n",
       "      <td>W</td>\n",
       "      <td>52</td>\n",
       "      <td>20.25154</td>\n",
       "      <td>18.99953</td>\n",
       "      <td>15.63252</td>\n",
       "      <td>4.86126</td>\n",
       "      <td>39.63625</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pm10</td>\n",
       "      <td>NHITS</td>\n",
       "      <td>W</td>\n",
       "      <td>78</td>\n",
       "      <td>29.27972</td>\n",
       "      <td>25.55316</td>\n",
       "      <td>30.36795</td>\n",
       "      <td>6.92467</td>\n",
       "      <td>91.43602</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pm10</td>\n",
       "      <td>NBEATS</td>\n",
       "      <td>W</td>\n",
       "      <td>12</td>\n",
       "      <td>23.12074</td>\n",
       "      <td>16.27287</td>\n",
       "      <td>12.99588</td>\n",
       "      <td>3.31098</td>\n",
       "      <td>23.21854</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pm10</td>\n",
       "      <td>NBEATS</td>\n",
       "      <td>W</td>\n",
       "      <td>26</td>\n",
       "      <td>24.88597</td>\n",
       "      <td>21.07951</td>\n",
       "      <td>15.33145</td>\n",
       "      <td>5.59960</td>\n",
       "      <td>54.09961</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pm10</td>\n",
       "      <td>NBEATS</td>\n",
       "      <td>W</td>\n",
       "      <td>52</td>\n",
       "      <td>21.21281</td>\n",
       "      <td>19.80284</td>\n",
       "      <td>16.39186</td>\n",
       "      <td>5.01307</td>\n",
       "      <td>43.11202</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pm10</td>\n",
       "      <td>NBEATS</td>\n",
       "      <td>W</td>\n",
       "      <td>78</td>\n",
       "      <td>31.24361</td>\n",
       "      <td>28.22053</td>\n",
       "      <td>36.15615</td>\n",
       "      <td>7.37499</td>\n",
       "      <td>104.68935</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pollutant  method freq   h      mape     smape       max      mae  \\\n",
       "11      pm10   NHITS    W  12  21.61003  15.47986  12.52267  3.15901   \n",
       "10      pm10   NHITS    W  26  23.89276  21.12368  25.26393  5.52537   \n",
       "6       pm10   NHITS    W  52  20.25154  18.99953  15.63252  4.86126   \n",
       "11      pm10   NHITS    W  78  29.27972  25.55316  30.36795  6.92467   \n",
       "12      pm10  NBEATS    W  12  23.12074  16.27287  12.99588  3.31098   \n",
       "9       pm10  NBEATS    W  26  24.88597  21.07951  15.33145  5.59960   \n",
       "9       pm10  NBEATS    W  52  21.21281  19.80284  16.39186  5.01307   \n",
       "12      pm10  NBEATS    W  78  31.24361  28.22053  36.15615  7.37499   \n",
       "\n",
       "          mse  smape_rank  mae_rank  mape_rank  mse_rank  max_rank  \n",
       "11   20.76394          12        12         12        12        12  \n",
       "10   58.60768          11         4          3        11        14  \n",
       "6    39.63625           7         2          4         1         7  \n",
       "11   91.43602          12        12         12        12         1  \n",
       "12   23.21854          13        13         13        13        13  \n",
       "9    54.09961          10         7          7         6         8  \n",
       "9    43.11202          10         7          8         6         9  \n",
       "12  104.68935          13        13         14        13        18  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for model in ['NHITS','NBEATS']:\n",
    "    for H in [12,26,52,78]:\n",
    "        x = joblib.load(fr'Results COBEQ/Stats (W)/pm10/{H}W_Df.pkl').sort_values(['smape','mae']).reset_index()\n",
    "        metrics = ['smape', 'mae', 'mape', 'mse', 'max']\n",
    "        for metric in metrics:\n",
    "            x[f'{metric}_rank'] = x[metric].rank(method='min')\n",
    "\n",
    "        # If you want best = 1, worst = 21, and ranks to be integers\n",
    "        x[[f'{m}_rank' for m in metrics]] = x[[f'{m}_rank' for m in metrics]].astype(int)\n",
    "        df = pd.concat([df, x[x['method'] == model]])\n",
    "        del df['index']\n",
    "df.to_excel(r'x.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for model in ['NHITS','NBEATS']:\n",
    "    for H in [12,26,52,78]:\n",
    "        x = joblib.load(fr'C:\\Users\\gustavo.filho\\Documents\\Python\\GitHub\\TCC\\Results COBEQ\\{model} (W)\\pm10\\{H}W_Df.pkl').sort_values(['smape','mae']).reset_index()\n",
    "        df = pd.concat([df, pd.DataFrame(x.iloc[0,:]).T])\n",
    "\n",
    "df.to_excel(r'x.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA, AutoCES, AutoETS, AutoTheta\n",
    "\n",
    "results_stats = []\n",
    "for pollutant in ['pm10']:\n",
    "    for h in [12, 26, 52, 78]:\n",
    "\n",
    "        obj = TimeObject(data, pollutant, agg_freq='D')\n",
    "        obj.NIXTLA_train_test(split=h)\n",
    "\n",
    "        season_length = 365 # Monthly data \n",
    "        horizon = len(obj.Y_train) # number of predictions\n",
    "\n",
    "        models = [\n",
    "            # AutoARIMA(season_length=season_length, alias='AutoARIMA'),\n",
    "            AutoCES(season_length=season_length, model='Z', alias='AutoCES-Z'),\n",
    "            AutoCES(season_length=season_length, model='S', alias='AutoCES-S'),\n",
    "            AutoCES(season_length=season_length, model='P', alias='AutoCES-P'),\n",
    "            AutoCES(season_length=season_length, model='N', alias='AutoCES-N'),\n",
    "            AutoTheta(season_length=season_length, decomposition_type=\"multiplicative\", alias='AutoTheta-Multi'),\n",
    "            AutoTheta(season_length=season_length, decomposition_type=\"additive\", alias='AutoTheta-Add'),\n",
    "        ]\n",
    "        models = models + [\n",
    "            AutoETS(season_length=season_length, model=ets, alias=f'AutoETS-{ets}')\n",
    "            for ets in [f\"{e}{t}{s}\" for e in ['Z',]\n",
    "                        for t in ['Z', 'A', 'N'] \n",
    "                        for s in ['Z', 'A', 'M', 'N'] \n",
    "        ]]\n",
    "\n",
    "        frct = StatsForecast(models=models, freq='D')\n",
    "        frct.fit(df=obj.Y_train)\n",
    "        predicted = frct.predict(h=h)\n",
    "\n",
    "        columns = predicted.columns\n",
    "        columns = columns[(columns != 'ds') & (columns != 'unique_id')]\n",
    "\n",
    "        results_stats = pd.DataFrame()\n",
    "        for method in columns:\n",
    "            obj.plot_forecast(predicted, show_metrics=True, method=method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pollutant in ['pm10']:\n",
    "    for h in [12, 26, 52, 78]:\n",
    "\n",
    "        obj = TimeObject(data, pollutant, agg_freq='D')\n",
    "        obj.NIXTLA_train_test(split=h)\n",
    "        horizon = len(obj.Y_train) # number of predictions\n",
    "\n",
    "        # ======================================================================================================\n",
    "\n",
    "        nbeats = joblib.load(fr\"Results COBEQ\\NBEATS (W)\\{pollutant}\\{h}W_Study.pkl\")\n",
    "        model = NBEATS(\n",
    "            h=h,\n",
    "            input_size=nbeats.best_trials[0].params.get('input_size'),\n",
    "            stack_types=nbeats.best_trials[0].params.get('interpretability')+(nbeats.best_trials[0].params.get('n_stacks')-len(nbeats.best_trials[0].params.get('interpretability')))*['identity'],\n",
    "            n_blocks=nbeats.best_trials[0].params.get('n_stacks') * [nbeats.best_trials[0].params.get('n_blocks')],\n",
    "            max_steps=nbeats.best_trials[0].params.get('max_steps'),\n",
    "            learning_rate=1e-3,\n",
    "            val_check_steps=10,\n",
    "        )\n",
    "        fcst = NeuralForecast(\n",
    "            models=[model],\n",
    "            freq='D',\n",
    "            local_scaler_type=nbeats.best_trials[0].params.get('local_scalar_type')\n",
    "        )\n",
    "        fcst.fit(df=obj.Y_train, verbose=False)\n",
    "        predicted = fcst.predict(df=obj.Y_train, verbose=False)\n",
    "        obj.plot_forecast(predicted, method='NBEATS', show_metrics=True)\n",
    "\n",
    "        # ======================================================================================================\n",
    "        \n",
    "        nhits = joblib.load(fr\"Results COBEQ\\NHITS (W)\\{pollutant}\\{h}W_Study.pkl\")\n",
    "        model = NHITS(\n",
    "            h=h,\n",
    "            input_size=nhits.best_trials[0].params.get('input_size'),\n",
    "            stack_types=nhits.best_trials[0].params.get('n_stacks')*['identity'],\n",
    "            n_freq_downsample=nhits.best_trials[0].params.get('n_freq_downsample')+(nhits.best_trials[0].params.get('n_stacks')-len(nhits.best_trials[0].params.get('n_freq_downsample')))*[1],\n",
    "            n_blocks=nhits.best_trials[0].params.get('n_stacks')*[nhits.best_trials[0].params.get('n_blocks')],\n",
    "            n_pool_kernel_size=(nhits.best_trials[0].params.get('n_stacks')-len(nhits.best_trials[0].params.get('n_pool_kernel_size')))*[1]+nhits.best_trials[0].params.get('n_pool_kernel_size'),\n",
    "            pooling_mode=\"MaxPool1d\",\n",
    "            activation=\"ReLU\",\n",
    "            interpolation_mode='linear',\n",
    "            max_steps=nhits.best_trials[0].params.get('max_steps'),\n",
    "            val_check_steps=10,\n",
    "        )\n",
    "        fcst = NeuralForecast(\n",
    "            models=[model],\n",
    "            freq='D',\n",
    "            local_scaler_type=nhits.best_trials[0].params.get('local_scalar_type')\n",
    "        )\n",
    "        fcst.fit(df=obj.Y_train, verbose=False)\n",
    "        predicted = fcst.predict(df=obj.Y_train, verbose=False)\n",
    "        obj.plot_forecast(predicted, show_metrics=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
